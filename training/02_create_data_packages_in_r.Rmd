# Create data packages in R

This chapter will teach you how to create and submit a data package to a DataONE node in R.

## What is in a package?

A data package generally consists of at least 3 "objects" or files. 

1. Metadata: One object is the metadata file itself. In case you are unfamiliar with metadata, metadata are information that describe data (e.g. who made the data, how were the data made, etc.). The metadata file will be in an XML format, and have the extension `.xml` (extensible markup language). We often refer to this file as the EML (Ecological Metadata Language), which is the metadata standard that it uses.

2. Data: Other objects in a package are the data files themselves. Most commonly these are data tables (`.csv`), but they can also be audio files, NetCDF files, plain text files, PDF documents, image files, etc. 

3. Resource Map: The final object is the resource map. This object is a plain text file with the extension `.rdf` ([Resource Description Framework](https://www.w3.org/RDF/)) that defines the relationships between all of the other objects in the data package. It says things like "this metadata file describes this data file," and is critical to making a data package render correctly on the website with the metadata file and all of the data files together in the correct place. Fortunately, we rarely, if ever, have to actually look at the contents of resource maps; they are generated for us using tools in R.

## About identifiers

Each object (metadata files, data files, resource maps) on the ADC or the KNB (another repo) has a unique identifier, also sometimes called a "PID" (persistent identifier). When you look at the landing page for a data set, for example [here](https://arcticdata.io/catalog/#view/doi:10.18739/A2836Z), you can find the resource map identifier listed under the title in the gray bar after the words "Files in this dataset Package:" (`resource_map_doi:10.18739/A2836Z`), the metadata identifier in the "General > Identifier" section of the metadata record or after the title with blue font (`doi:10.18739/A2836Z`), and the data identifier by clicking the "more info" link next to the data object, and looking at the "Online Distribution Info" section (`arctic-data.9546.1`).


![](images/PIDs.png)


Different versions of a package are linked together by what we call the "version chain" or "obsolescence chain". Making an update to a data package, such as replacing a data file, changing a metadata record, etc, will result in a new identifier for the new version of the updated object. When making changes to a package, always use the `arcticdatautils::update_object` or `arcticdatautils::publish_update` commands on the *latest versions* of all objects to ensure that the version chain is maintained.

## Uploading packages using R

We will be using R to connect to the [NSF Arctic Data Center (ADC)](https://arcticdata.io/catalog/#data) data repository to push and pull edits. To identify yourself as an admin you will need to pass a 'token' into R. Do this by signing in to the ADC with your [ORCiD](https://orcid.org/register), hovering over your name and clicking on "My profile", then navigating to "Settings" and "Authentication Token", copying the "Token for DataONE R", and pasting and running it in your R console.

**This token is your identity on these sites, please treat it as you would a password** (ie. don't paste into scripts that will be shared). The easiest way to do this is to always run the token in the *console*. There's no need to keep it in your script since it's temporary anyway. Sometimes you'll see a placeholder in scripts to remind users to get their token, such as:

```{r token, message=FALSE, eval=FALSE}
options(dataone_test_token = "...")
```

Next, please be sure these packages are loaded:
```{r load libraries, message=FALSE}
library(devtools)
library(dataone)
library(datapack)
library(EML)
library(remotes)
library(XML)
```

If any package could not be loaded, use the following command (replacing package_name with the actual package name) to install the package, then load them.

```{r, eval = F}
install.packages("package_name")
```

Now install a couple of packages:
```{r, eval = F}
remotes::install_github("nceas/arcticdatautils"); library(arcticdatautils)
remotes::install_github("nceas/datamgmt"); library(datamgmt)
```

For this training, we will be working exclusively on the Arctic test site, or "node." Set the node to the test Arctic node:

``` {r, eval = F, class.source = 'exercise'}
cnT <- dataone::CNode('STAGING')
mnT <- dataone::getMNode(cn,'urn:node:mnTestARCTIC')
```

```{r, child = '../workflows/edit_data_packages/publish_an_object.Rmd'}
```

```{r, child = '../workflows/edit_data_packages/create_a_resource_map.Rmd'}
```

## SASAP package workflows
Sometimes many data sets are associated with a larger project, such as the State of Alaska Salmon and People (SASAP) project. These data sets should be given additional project-specific information using `eml@dataset@project`. This will add pre-defined information including the project title, funding sources, and key personnel. You will also want to set access permissions to the project as well. If you are working on a SASAP data set, prior to writing the EML and publishing the data set you will set the project with this code:

```{r, eval=FALSE}
source('~/sasap-data/data-submission/Helpers/SasapProjectCreator.R')
eml@dataset@project <- sasap_project()
```

Next you should add SASAP-specific taxonomic coverage to the EML using the `add_SASAP_taxa` function found in `sasap-data/data-submission/Helpers`. This will ensure that any salmon species present in the dataset have their Latin and common names listed in the coverage and can be easily searched for. Run everything in the `add_SASAP_taxa.R` script prior to running `eml <- add_SASAP_taxa(eml)`.

Then, update the access permissions in the system metadata using `set_rights_and_access`.

```{r, eval=FALSE}
pkg <- get_package(mn, resource_map_pid)
set_rights_and_access(mn, unlist(pkg), 'CN=SASAP,DC=dataone,DC=org', permissions = c('read', 'write', 'changePermission'))
```

Finally, go through the [SASAP checklist](https://github.nceas.ucsb.edu/NCEAS/sasap-data/blob/master/data-submission/checklist.Rmd) to ensure that the package meets all of the project-specific requirements for publishing.

After your package is published, run the `qa_package()` function from the `datamgmt` package. 

The function arguments are as follows:
```{r, eval=FALSE}
qa_package <- function(node, pid, readAllData = TRUE,
                       check_attributes = TRUE,
                       check_creators = FALSE,
                       check_access = FALSE)
```

By default, `qa_package` checks for:
- Correctness of distribution URLs for each data object (URLs must match the EML physical section for the object)
- Congruence of metadata and data 

The `check_creators` and `check_access` flags can be set to `TRUE` to check: 
- Correctness of ORCIDs of creators in a given EML
- Rights and access are set for creators for a give pid sysmeta

In most cases, QA package will be run just by passing in a member node and the resource map pid.
```{r, eval=FALSE}
qa_package(mn, resource_map_pid)
```

Note that there are some issues with the function that are documented [here](https://github.com/NCEAS/datamgmt/issues/145). 

## Exercise 2 {.exercise}

* Locate the data package you published in [Exercise 1](#exercise-1) by navigating to the "My Profile > My Data" section on [test.arcticdata.io](https://test.arcticdata.io).
* Download the metadata and data files and transfer them to the Datateam server.
* Using the functions described in the section above, publish your metadata record and data file to the site. (When you do so, make sure you save the `PID`s to different variables in R...) i.e.

``` {r, eval = FALSE, class.source = 'exercise'}
data_pid <- arcticdatautils::publish_object(...)
metadata_pid <- arcticdatautils::publish_object(...)
```

* Then create a resource map containing your data and metadata.
* View your new data set (which is identical to the one you created previously) by appending the metadata PID to the end of the URL test.arcticdata.io/#view/... 

[
["index.html", "NCEAS Data Team Training Chapter 1 Welcome to NCEAS! 1.1 First day to-dos 1.2 Account information 1.3 NCEAS Events", " NCEAS Data Team Training Jeanette Clark, Jesse Goldstein, Dominic Mullen, Bryce Mecum 2017-12-14 Chapter 1 Welcome to NCEAS! 1.1 First day to-dos Get a tour of the office from Ginger, Jeanette, or Jesse Fill out required paperwork from Michelle Have Ana take your picture (room 309) Set up the remainder of your accounts 1.2 Account information LDAP - NCEAS - Jeanette or Jesse should have set this up prior to your start date to help get other accounts set up by the first day. This account and password control your access to: arcticdata RT queue Github - arctic-data and sasap-data Datateam server - follow instructions in email from Nick at NCEAS to reset your datateam password in the terminal ORCiD - create an account NCEAS Slack - get an invite from slack.nceas.ucsb.edu Arctic Data Center Team - after creation of ORCiD and sign-in to both arcticdata.io and test.arcticdata.io, request an add from Chris on Slack Schedule - fill out anticipated quarterly schedule 1.3 NCEAS Events NCEAS hosts a number of events that you are encouraged to attend. Keep an eye on your email but the weekly events are : Roundtable weekly presentation and discussion of research by a visiting or local scientist Wednesdays at 12:15 in the lounge Coffee Klatch coffee, socializing, and news updates for NCEAS Tuesdays at 10:30 in the lounge Salad Potluck potluck salad and socializing, bring a topping or side to share! second Tuesday of the month, 12:15 in the lounge "],
["introduction-to-open-science.html", "Chapter 2 Introduction to open science 2.1 Open science background reading 2.2 Effective data management 2.3 Using DataONE 2.4 Working on a remote server", " Chapter 2 Introduction to open science These materials are meant to introduce you to the principles of open science, effective data management, and data archival with the DataONE data repository. New data team members should complete the exercise in this section as part of training. 2.1 Open science background reading Read the content on the Arctic Data Center (ADC) webpage to learn more about data submission, preservation, and the history of the ADC about submission preservation history 2.2 Effective data management Read Matt Jones’ paper on effective data management to learn how we will be organizing datasets prior to archival. 2.3 Using DataONE The Data Observation Network for Earth (DataONE) is a community driven project providing access to data across multiple member repositories, supporting enhanced search and discovery of Earth and environmental data. Watch the first 38 minutes of this video explaining how DataONE works. This video is pretty technical, and you may not understand it all at first. Please feel free to ask Jesse or Jeanette questions. 2.4 Working on a remote server All of the work that we do at NCEAS is done on our remote server, called “datateam.” If you have never worked on a remote server before, you can think of it like working on a different computer via the internet. We access RStudio for our server through the link datateam.nceas.ucsb.edu/rstudio/. To transfer files on and off of the server, you’ll need to use either bash commands in the terminal, or a program called cyberduck. 2.4.1 Cyberduck instructions To use cyberduck to transfer local files onto the datateam server: Open Cyberduck Click “Open Connection” From the drop-down, choose “SFTP (SSH File Transfer Protocol)” Enter “datateam.nceas.ucsb.edu” for Server Enter your username and password Connect From here, you can drag and drop files onto and off of the server. 2.4.2 A note on paths On the servers, paths to files in your folder always start with /home/yourusername/.... When you write scripts, try to avoid writing relative paths (which rely on what you have set your working directory to) as much as possible. Instead, write out the entire path as shown above, so that if another intern or Jesse or Jeanette need to run your script, it is not dependent on a working directory. 2.4.3 Exercise Use the “tabula” program to extract Table 1 from this paper Reformat the table using R under the guidelines in the journal article on effective data management If you need an R refresher, take as much time as you need to go over the data carpentry guide You may also find the data carpentry lesson on dplyr helpful Go to test.arcticdata.io and submit your reformatted file with appropriate metadata that you derive from the text of the paper list yourself as the first creator so your test submission can easily be found for the purposes of this training exercise, not every single author needs to be listed with full contact details, listing the first two authors is fine attributes (column names) should be defined, including correct units and missing value codes "],
["using-arcticdatautils.html", "Chapter 3 Using arcticdatautils 3.1 Uploading packages using R 3.2 Uploading a new version", " Chapter 3 Using arcticdatautils New data team members should complete exercises two and three as part of training. 3.1 Uploading packages using R We will be using the arcticdatautils package and the dataone package to connect to the NSF Arctic Data Center data repository and push and pull edits. To identify yourself as admin you will need to register a ‘token’ by signing in to the ADC with your ORCiD, copying your token into R and telling R where to connect by defining the member node using the code below. library(arcticdatautils) library(dataone) 3.1.1 Set environment Once we have our libraries loaded we need to tell R which repo we want to be working with. If we are working on the production site, you can set the coordinating node (cn) and member node (mn) this way: cn &lt;- CNode(&#39;PROD&#39;) mn &lt;- getMNode(cn,&#39;urn:node:ARCTIC&#39;) If we are working in the test environment, we set it this way: cn &lt;- CNode(&#39;STAGING&#39;) mn &lt;- getMNode(cn,&#39;urn:node:mnTestARCTIC&#39;) A note on nodes - be very careful about what you publish on the production node (PROD, or arcticdata.io). This node should NEVER be used to publish test or training datasets. While going through training exercises, you should be using the test environment (STAGING, or test.articdata.io). 3.1.2 Set token Now that we have the environment set we have to manually log in to the NSF ADC site to get our token and copy it to our R environment. This token is your identity on these sites, please treat it as you would a password (ie. don’t paste into scripts that will be shared). The easiest way to do this is to just run the token in the console. There’s no need to keep it in you script since its temporary anyway. Sometimes you’ll see a placeholder in scripts to remind users to get their token, such as: options(dataone_test_token = &quot;...&quot;) 3.1.3 What is in a package? A data package generally consists of at least 3 “objects” or files. One object is the metadata file itself. Ths file is in XML format, and has the extension .xml. We sometimes refer to this file as the EML, which is the metadata standard that it uses. Another object or objects are the data files themselves. Most commonly these are data tables (.csv), but they can also be audio files, netCDF files, plain text files, pdf documents, image files, etc. The final object is the resource map. This object is a text file that defines the relationships between all of the other objects in the data package. It says things like “this metadata file describes this data file,” and is critical to making a data package render correctly on the website with the metadata files and all of the data files in te right place. Fortunately, we rarely if ever have to actually look at the contents of resource maps, they are generated for us using arcticdatautils. 3.1.4 About identifiers Each object (metadata files, data files, resource maps) on the Artic Data Center or KNB has a unique identifier, also sometimes called a “pid”. When you look at the landing page for a dataset, for example here, you can find the resource map identifier in the URL (resource_map_doi:10.18739/A2836Z), the metadata identifer in the “General &gt; Identifier” section of the metadata record (doi:10.18739/A2836Z), and the data identifier by clicking the “more info” link next to the data object, and looking at the “online distribution info” section (arctic-data.9546.1). Different versions of a package are linked together by what we call the version chain. Making an update to a data package, such as replacing a data file, changing a metadata record, etc, will result in a new identifier for the object that was updated. When making changes to a package, always use the update_object or publish_update commands from arcticdatautils to ensure that the version chain is maintained. 3.1.5 Publishing a package to the site Objects are published to the site using a function called publish_object. To publish a metadata file you would use the following call: meta_path &lt;- &#39;path/to/your/metadata/metadata.xml&#39; pid &lt;- publish_object(mn, meta_path, format_id = format_eml()) This call will save the id of the metadata object to the variable pid, which you will need later for the resource map. It also adds a formatID, which identifies what kind of object you just uploaded. Similarly, you can publish data objects like this: data_path &lt;- &#39;path/to/your/data/data.csv&#39; data_pid &lt;- publish_object(mn, data_path, format_id = &#39;text/csv&#39;) Note that you will need to be explicit about your format_id here based on the file type. A list of format IDs can be found here on the DataONE website. Finally, you can create and publish the resource map using create_resource_map. rm &lt;- create_resource_map(mn, pid, data_pids = data_pid) One final step is to make sure that the rights and access on thes objects you uploaded are set correctly. The function set_rights_and_access will set both, and set_access will just set access. There are two functions for this because a rights holder should always have access, but not all people who need access are rights holders. The rights holder to the dataset is typically the submitter (if the dataset is submitted through the registry), but if a data team intern is publishing objects for a PI, the rights holder is the main point of contact for the dataset (ie the person who requested that we upload the data for them). In this example, we set rights and access for all of the objects created above, using an example ORCID. set_rights_and_access(mn, c(pid, data_pid, rm), subject = &#39;http://orcid.org/PUT0-YOUR-ORCD-HERE&#39;, permissions = c(&#39;read&#39;, &#39;write&#39;, &#39;changePermission&#39;)) 3.1.6 Exercise Locate the data package you published by navigating to the “my profile” section on test.arcticdata.io. Download the metadata and data files and transfer them to the datateam server. Using the functions described in the section above, pubish your metadata record and data file to the site. View your new dataset (which is identical to the one you created previously) by appending the pid of your resource map (in the example above, the resource map pid was saved to the variable rm) to the end of the URL test.arcticdata.io/#view/… 3.2 Uploading a new version To edit material, we use the publish_update or update_object functions in arcticdatautils. Usually we’ll be updating exisiting work using publish_update() from the arcticdatautils package, which has an argument for adding data PIDs (or otherwise including existing data PIDs to make sure that they stay with the package). This function allows you to add or remove data objects, and/or make metadata edits. The following example shows how you would add a data file to a package, and make a change to the metadata file. First, make whatever edits you ned to make to the metadata, and save them to the server. Define the path to your metadata. meta_path &lt;- &quot;path/to/metadata/file.xml&quot; Use either the metadata or resource map pid for your existing data package and the function get_package to return all of the existing pids in the data package as a named list. # the function get_package is helpful to get all other PIDs in a package if you know the metadata PID metaPid &lt;- &quot;urn:uuid:5933a052-5493-4f50-8622-ed83231d3fee&quot; ids &lt;- get_package(mn, metaPid) Finally, use the publish_update function with the results of the lines above to update your data package. publish_update(mn, metadata_pid = metaPid # old metadata PID you pulled in originally. Alternatively could paste in the string: &quot;urn:uuid:a592a9a2-547c-48ee-bff2-add133aa64ee&quot; resource_map_pid = ids$resource_map, metadata_path = meta_path, # path to updated EML file, data_pid = ids$data, # this can also be 2+ by including the PIDs in a vector c(pid1, pid2,pid3) check_first = T, use_doi = FALSE, public = FALSE) If a package is ready to be public, note that you can change the public argument in the publish_update call to TRUE. Similarly, if you want to publish with a DOI type identifier instead of a UUID type identifer, you can change the use_doi argument to TRUE. 3.2.1 Exercise Locate the data package you published in the previous exercise by navigating to the URL test.arcticdata.io/#view/… with the resource map pid you generated in that exercise. Download the EML to your computer, and open it in a plain text editor (TextEdit or Atom). Navigate to the title section, and make a change to the title text within the &lt;title&gt;…&lt;/title&gt; section. Do not change the &lt;title&gt; or &lt;/title&gt; syntax. Upload the file back to the datateam server. Using the publish_update function, update your data package as shown above. "],
["system-metadata.html", "Chapter 4 System metadata 4.1 Introduction 4.2 Editing sysmeta 4.3 Editing sysmeta on lots of files 4.4 Identifiers and sysmeta", " Chapter 4 System metadata New data team members should complete the exercise in this section as part of training. 4.1 Introduction Every object in the Arctic Data Center (or on the KNB) has “system metadata.” An object’s system metadata has information about the file itself, such as the name of the file, the format, who the rights holder is, and what the access policy is (amongst other things). Sometimes we will need to edit system metadata in order to make sure that things on the webpage display correctly, or to ensure a file downloads from the website with the correct file name and extension. Although the majority of system metadata changes that need to be made are done with the functions in arcticdatautils, sometimes we need to change aspects of the system metadata (or sysmeta) outside of those functions. This markdown explains how to do this using the functions getSystemMetadata and updateSystemMetadata. 4.2 Editing sysmeta First we need to load in some packages, and set up our environment. Note that typically if you are editing sysmeta you will need to set a token just like if you were updating a package. library(arcticdatautils) library(dataone) cn &lt;- CNode(&#39;STAGING&#39;) mn &lt;- getMNode(cn,&#39;urn:node:mnTestARCTIC&#39;) #Loading in the test environment for this example Next, we input the mn instance and PID of interest into the getSystemMetadata function. Note that the PID should be of the object of interest, not just the package of interest. Each individual object (metadata file, resource map, data file) will have its own system metadata. pid &lt;- &#39;urn:uuid:9a1b02a8-713e-4682-aafb-95c854a4c24a&#39; sysmeta &lt;- getSystemMetadata(mn, pid) This returns an S4 object (more on these objects in Chapter 4), called sysmeta. The sysmeta object has the following “slots”: * serial version * identifier * formatId * checksum * checksumAlgorithm * submitter * rightsHolder * accesPolicy * replicationAllowed * numberReplicas * preferredNodes * blockedNodes * obsoletes * obsoletedBy * archived * dateUploaded * dateSysMetaModified. You can view and edit slots using the @ functionality. sysmeta@fileName [1] &quot;WELTS_flatfile.csv&quot; If you want to change a slot, you can simply do the following: sysmeta@fileName &lt;- &#39;AlaskaWells.csv&#39; Note that some slots cannot be changed by simple text replace (particularly the accessPolicy). There are various helper functions for changing the accessPolicy and rightsHolder such as addAccessRule (which takes the sysmeta as an input) or set_access, which only requires a PID. In general, you most frequently need to use getSystemMetadata to change either the formatId or fileName slots. After you have changed the necessary slot, you can update the system metadata using the updateSystemMetadata function, which takes the mn instance, your PID of interest, and the edited sysmeta object as arguments. updateSystemMetadata(mn, pid, sysmeta) 4.2.1 Exercise Read the system metadata in from the data file you uploaded in the previous chapter Check to make sure the fileName and formatId are set correctly Update the system metadata if necessary. 4.3 Editing sysmeta on lots of files If you have a lot of files that all need a similar change, you can use a for loop or the apply functions to efficiently make all of your changes. Say you need to change all of the format IDs of data objects in a package so that they are text/csv. You could do this: PID &lt;- &#39;some_pid&#39; ids &lt;- get_package(mn, PID) #get all pids in a package data_pids &lt;- unlist(ids$data) for (i in 1:length(data_pids)){ #run for loop over all of the data ids sysmeta &lt;- getSystemMetadata(mn, data_pids[i]) sysmeta@formatId &lt;- &quot;text/csv&quot; updateSystemMetadata(mn, data_pids[i], sysmeta) } 4.4 Identifiers and sysmeta Importantly, changing the system metadata does NOT necessitate a change in the PID of an object. This is because changes to the system metadata do not change the object itself, they are only changing the description of the object (although ideally the system metadata is accurate when an object is first published). "],
["editing-eml-in-r.html", "Chapter 5 Editing EML in R 5.1 Introduction 5.2 Making edits 5.3 Validating and writing the EML 5.4 Dealing with unusual cases 5.5 Exercise", " Chapter 5 Editing EML in R New data team members should complete the exercise in this section as part of training. 5.1 Introduction This chapter is a practical tutorial for using the EML package to read, edit, write, and validate EML documents. Much of this information can be also be found in the vignettes for the EML package. First, load in some packages. library(EML) library(XML) library(digest) library(arcticdatautils) Set a path to a local copy of the EML that you would like to edit, and read the EML into R. path1 &lt;- &#39;glacier_metadata.xml&#39; eml &lt;- read_eml(path1) When using the EML R package, you will usually be working with objects of class eml. For example, we can find out what type of object we just created when we ran the read_eml function: class(eml) ## [1] &quot;eml&quot; ## attr(,&quot;package&quot;) ## [1] &quot;EML&quot; This eml object represents a complete EML document (as in: the .xml file) and all of the information inside the .xml file can be viewed and edited with the EML package. The eml object has what are called “slots,” each slot representing an element of the EML document such as “Title” or “Creator” and you will use these “slots” when working with your eml object. You can find out what slots you have available with the slotNames function: sort(slotNames(eml)) ## [1] &quot;.Data&quot; &quot;access&quot; &quot;additionalMetadata&quot; ## [4] &quot;citation&quot; &quot;dataset&quot; &quot;lang&quot; ## [7] &quot;packageId&quot; &quot;protocol&quot; &quot;schemaLocation&quot; ## [10] &quot;scope&quot; &quot;slot_order&quot; &quot;software&quot; ## [13] &quot;system&quot; You can access information in one of these slots by adding an @ symbol at the end of the variable you want to view the slots of and hitting TAB (e.g., eml@&lt;TAB&gt;): RStudio Autocompletion You will notice that the list of slots on the eml object doesn’t include common EML things such as the dataset’s title or the creators. This is because EML (the XML) and the EML R package are both hierarchical. The (incomplete) EML XML for dataset with a title would look something like this: &lt;eml&gt; &lt;dataset&gt; &lt;title&gt;My title goes here...&lt;/title&gt; &lt;/dataset&gt; &lt;/eml You can see that the title is nested inside the dataset and the dataset is nested inside the root eml element of the document. Because the title is nested inside the dataset element, it will be a slot on eml@dataset instead of an slot of the main eml object: sort(slotNames(eml@dataset)) ## [1] &quot;.Data&quot; &quot;abstract&quot; &quot;additionalInfo&quot; ## [4] &quot;alternateIdentifier&quot; &quot;associatedParty&quot; &quot;contact&quot; ## [7] &quot;coverage&quot; &quot;creator&quot; &quot;dataTable&quot; ## [10] &quot;distribution&quot; &quot;id&quot; &quot;intellectualRights&quot; ## [13] &quot;keywordSet&quot; &quot;lang&quot; &quot;language&quot; ## [16] &quot;maintenance&quot; &quot;metadataProvider&quot; &quot;methods&quot; ## [19] &quot;otherEntity&quot; &quot;project&quot; &quot;pubDate&quot; ## [22] &quot;publisher&quot; &quot;pubPlace&quot; &quot;purpose&quot; ## [25] &quot;references&quot; &quot;schemaLocation&quot; &quot;scope&quot; ## [28] &quot;series&quot; &quot;shortName&quot; &quot;slot_order&quot; ## [31] &quot;spatialRaster&quot; &quot;spatialVector&quot; &quot;storedProcedure&quot; ## [34] &quot;system&quot; &quot;title&quot; &quot;view&quot; Remember, you can just type eml@dataset into your console and hit to see this list of slot names. The slotNames function is used for demonstration purposes here. Slots can be nested in each other and are all based on the EML schema. Typing in the name of your eml object (in this case, eml, the name of the result of read_eml) and hitting &lt;RETURN&gt; in the console will print the entire EML onto the screen: eml Similarly, you can also view different elements of the EML by drilling down into the structure using the @ functionality. This would just print the dataset portion of the EML: eml@dataset Going even deeper, this command will print the title element, which is within the dataset element, of the EML document: eml@dataset@title ## An object of class &quot;ListOftitle&quot; ## [[1]] ## &lt;title&gt;Subglacial conduit fluid dynamics simulation, Svalbard, Norway&lt;/title&gt; Notice than when the last line prints, it doesn’t just print a string - it returns “An object of class ListOftitle.” The EML is composed of different classes of objects that make up these slots. If a slot can have multiple items, it may require an input of a list of a class. You can continue digging down into the EML using subsetting techniques for the S4 object class, with syntax that looks like this: eml@dataset@title@.Data[[1]]@.Data ## [1] &quot;Subglacial conduit fluid dynamics simulation, Svalbard, Norway&quot; Notice that this actually prints a character string. If you want to change the title, you could just assign this a new character string value, like this: eml@dataset@title@.Data[[1]]@.Data &lt;- &#39;This is my new title&#39; However, this isn’t best method to edit the EML unless you are an expert both in S4 objects and in the EML schema, since the nesting and lists of elements can get very complex. Instead, the EML package is used to create new objects of particular classes to put into the document, using the function new. The new function has a format that looks like newobject &lt;- new('class', arguments...). It can be a hard function to use because “class” has to be set to a specific name and the argument structure will vary depending on what the slots are in that class. Because the function is so general, the ?new help is not very helpful. In the EML package, a good guess to what the name of the class will be is a slot name (such as “title”). You can explore what slots are available within an object class by creating a new, empty object like this: test_title &lt;- new('title') and using the R autocomplete functionality on test_title@. Not that the slots lang, slot_order, schemaLocation, and .Data will always be present, and are set by the EML package automatically according to the required EML and XML schema. The other options will tell you what the arguments following the class name should be. In the case of the title class, the only option is value. So, to change the title, you might at first try do something like this: eml@dataset@title&lt;- new('title', .Data = 'This is my new title') Unfortunately it returns an error. The error reads: Error in (function (cl, name, valueClass) :assignment of an object of class “title” is not valid for @‘title’ in an object of class “dataset” ; is(value, &quot;ListOftitle&quot;) is not TRUE The error says it cannot assign an object of class “title” to eml@dataset@title, and that there is not a value for ListOftitle. So this means that you have to assign to @title an object of class ListOftitle, which is composed of objects of class title. Even though ListOftitle is it’s own kind of class, we can avoid using the new function again here by simply putting the title object in a vector using the c() function. title &lt;- new(&#39;title&#39;, .Data = &#39;This is my new title&#39;) eml@dataset@title &lt;- c(title) This seems kind of cumbersome, creating first a new title object, and then a list of title, especially since in most cases an EML will only have one title. However, say for some reason you need two titles - you can then do this: title_second &lt;- new(&#39;title&#39;, .Data = &#39;This dataset has two titles&#39;) eml@dataset@title &lt;- c(title, title_second) eml@dataset@title ## An object of class &quot;ListOftitle&quot; ## [[1]] ## &lt;title&gt;This is my new title&lt;/title&gt; ## ## [[2]] ## &lt;title&gt;This dataset has two titles&lt;/title&gt; This functionality will prove very useful with other elements (such as dataTable), where there is usually more than one of the same element. 5.2 Making edits 5.2.1 Setting attributes Since attribute information has to be added to the metadata, we’ll cover attributes first. 5.2.1.1 Building the attribute table First you need to generate a dataframe with attribute information. This dataframe has rows that are attributes, and the following columns: attributeName: The name of the attribute as listed in the csv. Required. attributeDefinition: Longer description of the attribute. Required. measurementScale: One of: nominal, ordinal, dateTime, ratio, interval. Required. nominal: unordered categories or text. eg: (Male, Female) or (Yukon River, Kuskokwim River) ordinal: ordered categories. eg: Low, Medium, High dateTime: date or time values from the Gregorian calendar. eg: 01-01-2001 ratio: measurement scale with a meaningful zero point. eg: 200 Kelvin is half as hot as 400 Kelvin, 1.2 metersPerSecond is twice as fast as 0.6 metersPerSecond. interval: values from a scale with equidistant points, where the zero point is arbitrary. eg: 12.2 degrees Celsius, 21 degrees Latitude domain: One of: textDomain, enumeratedDomain, numericDomain, dateTimeDomain. Required. textDomain: text that is free-form, or matches a pattern enumeratedDomain: text that belongs to a defined list of codes and definitions. eg: CASC = Cascade Lake, HEAR = Heart Lake dateTimeDomain: dateTime attributes numericDomain: attributes that are numbers (either ratio or interval) formatString: Required for dateTimeDomain, NA otherwise. Format string for dates, eg “MM/DD/YYYY”. definition: Required for textDomain, NA otherwise. Definition for attributes that are a character string, matches attribute definition in most cases. unit: Required for numericDomain, NA otherwise. Unit string. If the unit is not a standard unit, a warning will appear when you create the attribute list, saying that it has been forced into a custom unit. Use caution here to make sure the unit really needs to be a custom unit. A list of standard units can be found here: https://knb.ecoinformatics.org/#external//emlparser/docs/eml-2.1.1/./eml-unitTypeDefinitions.html#StandardUnitDictionary numberType: Required for numericDomain, NA otherwise. Options are “real”, “natural”, “whole”, “integer”. real: positive and negative fractions and non fractions (…-1,-0.25,0,0.25,1…) natural: non-zero positive counting numbers (1,2,3…) whole: positive counting numbers and zero (0,1,2,3…) integer: positive and negative counting numbers and zero (…-2,-1,0,1,2…) missingValueCode: Code for missing values (eg: ‘-999’, ‘NA’, ‘NaN’). NA otherwise. Note that an NA missing value code should be a string, ‘NA’, and numbers should also be strings, ‘-999.’ missingValueCodeExplanation: Explanation for missing values, NA if no missing value code exists. attributes1 &lt;- data.frame( attributeName = c(&#39;Date&#39;, &#39;Location&#39;, &#39;Region&#39;,&#39;Sample_No&#39;, &#39;Sample_vol&#39;, &#39;Salinity&#39;, &#39;Temperature&#39;, &#39;sampling_comments&#39;), attributeDefinition = c(&#39;Date sample was taken on&#39;, &#39;Location code representing location where sample was taken&#39;,&#39;Region where sample was taken&#39;, &#39;Sample number&#39;, &#39;Sample volume&#39;, &#39;Salinity of sample in PSU&#39;, &#39;Temperature of sample&#39;, &#39;comments about sampling process&#39;), measurementScale = c(&#39;dateTime&#39;, &#39;nominal&#39;,&#39;nominal&#39;, &#39;nominal&#39;, &#39;ratio&#39;, &#39;ratio&#39;, &#39;interval&#39;, &#39;nominal&#39;), domain = c(&#39;dateTimeDomain&#39;, &#39;enumeratedDomain&#39;,&#39;enumeratedDomain&#39;, &#39;textDomain&#39;, &#39;numericDomain&#39;, &#39;numericDomain&#39;, &#39;numericDomain&#39;, &#39;textDomain&#39;), formatString = c(&#39;MM-DD-YYYY&#39;, NA,NA,NA,NA,NA,NA,NA), definition = c(NA,NA,NA,&#39;Sample number&#39;, NA, NA, NA, &#39;comments about sampling process&#39;), unit = c(NA, NA, NA, NA,&#39;milliliter&#39;, &#39;dimensionless&#39;, &#39;celsius&#39;, NA), numberType = c(NA, NA, NA,NA, &#39;real&#39;, &#39;real&#39;, &#39;real&#39;, NA), missingValueCode = c(NA, NA, NA,NA, NA, NA, NA, &#39;NA&#39;), missingValueCodeExplanation = c(NA, NA, NA,NA, NA, NA, NA, &#39;no sampling comments&#39;), stringsAsFactors = FALSE) Typing this out in R can be a bit of a pain, so you can import a table made in another program (such as Excel) as your attribute table - just make sure that rows are attributes and column names match the column names as listed above exactly (case is important). attributes1 &lt;- read.csv('~/arctic-data/docs/training/EML/LakeSampleData.csv', stringsAsFactors = F) 5.2.1.2 Defining enumerated domains For attributes that are enumerated domains, a second table is needed with three columns: attributeName, code, and definition. attributeName is repeated for all codes belonging to a common attribute. To make things a little easier and less repetitve, coding wise, codes can be defined using named character vectors and then converted to a data frame. In this example, there are two enumerated domains in the attribute list - “Location” and “Region” Location &lt;- c(CASC = &#39;Cascade Lake&#39;, CHIK = &#39;Chikumunik Lake&#39;, HEAR = &#39;Heart Lake&#39;, NISH = &#39;Nishlik Lake&#39; ) Region &lt;- c(W_MTN = &#39;West region, locations West of Eagle Mountain&#39;, E_MTN = &#39;East region, locations East of Eagle Mountain&#39;) The definitions are then written into a dataframe using the names of the named character vectors, and their definitions. factors1 &lt;- rbind(data.frame(attributeName = &#39;Location&#39;, code = names(Location), definition = unname(Location)), data.frame(attributeName = &#39;Region&#39;, code = names(Region), definition = unname(Region))) factors1 ## attributeName code definition ## 1 Location CASC Cascade Lake ## 2 Location CHIK Chikumunik Lake ## 3 Location HEAR Heart Lake ## 4 Location NISH Nishlik Lake ## 5 Region W_MTN West region, locations West of Eagle Mountain ## 6 Region E_MTN East region, locations East of Eagle Mountain This table can also be generated using a different program, such as Excel, and imported to R as a .csv, similar to what can be done with the attribute table. 5.2.1.3 Generating the attribute list and data table Next the attributeList is generated from the attributes and the factors using the function set_attributes. This puts all of the information from the attribute data.frame and the factor data.frame defining the enumerated domains into the slotted EML schema. attributeList1 &lt;- set_attributes(attributes1, factors = factors1) Now the physical aspects of the data table, like its name, identifier (PID), header lines, and delimiter, need to be described. The function set_physical does this. See ?set_physical for more options on what can be set in the physical element. One of the more important items to set here is the URL, which points to the newest version of the data object using the object’s PID. id1 &lt;- &#39;PID1&#39; #this should be an actual PID path &lt;- &#39;~/arctic-data/docs/training/EML/LakeSampleData.csv&#39; #path to data table physical1 &lt;- set_physical(&#39;LakeSampleData.csv&#39;, id = id1, size = as.character(file.size(path)), sizeUnit = &#39;bytes&#39;, authentication = digest(path, algo=&quot;sha1&quot;, serialize=FALSE, file=TRUE), authMethod = &#39;SHA-1&#39;, numHeaderLines = &#39;1&#39;, fieldDelimiter = &#39;,&#39;, url = paste0(&#39;https://cn.dataone.org/cn/v2/resolve/&#39;, id1)) If the object is already on the Arctic Data Center, the physical section is very easy to write using its PID and the pid_to_eml_physical function: id1 &lt;- &#39;PID1&#39; #this should be an actual PID cn &lt;- CNode(&#39;PROD&#39;) mn &lt;- getMNode(cn,&#39;urn:node:ARCTIC&#39;) physical1 &lt;- pid_to_eml_physical(mn, id1) REMEMBER, if we are working in the test environment, we set it this way: cn &lt;- CNode(&#39;STAGING&#39;) mn &lt;- getMNode(cn,&#39;urn:node:mnTestARCTIC&#39;) Reminder: Be very careful about what you publish on the production node (PROD, or arcticdata.io). This node should NEVER be used to publish test or training datasets. While going through training for the first time you should be using the test environment (STAGING, or test.articdata.io). The physical1 and attributeList1 elements are then used to create the dataTable, along with the name of the dataTable and its description. dataTable1 &lt;- new(&#39;dataTable&#39;, entityName = &#39;LakeSampleData.csv&#39;, entityDescription = &#39;Water sample temperature and salinity from the Eagle Mountain region&#39;, physical = physical1, attributeList = attributeList1) 5.2.1.4 Adding a second dataTable If the metadata document describes multiple Data Objects, a new set of attributes, attribute list, physical description, and dataTable can be created just as in the example above. attributes2 &lt;- data.frame(attributeName = c(&#39;Time&#39;, &#39;Wind_Speed&#39;), attributeDefinition = c(&#39;Date and time of wind speed reading&#39;, &#39;Measured wind speed&#39;), measurementScale = c(&#39;dateTime&#39;, &#39;ratio&#39;), domain = c(&#39;dateTimeDomain&#39;, &#39;numericDomain&#39;), formatString = c(&#39;YYYY-MM-DD hh:mm:ss&#39;, NA), definition = c(NA, NA), unit = c(NA, &#39;metersPerSecond&#39;), numberType = c(NA, &#39;real&#39;), missingValueCode = c(NA, NA), codeExplanation = c(NA, NA), stringsAsFactors = FALSE) attributeList2 &lt;- set_attributes(attributes2) id2 &lt;- &#39;PID2&#39; physical2 &lt;- pid_to_eml_physical(mn, id2) dataTable2 &lt;- new(&#39;dataTable&#39;, entityName = &#39;EagleMtnWindData.csv&#39;, entityDescription = &#39;Wind data from Eagle Mountain&#39;, physical = physical2, attributeList = attributeList2) Now both dataTable1 and dataTable2 are added to the original EML using the c() function. eml@dataset@dataTable &lt;- c(dataTable1, dataTable2) 5.2.1.5 Defining custom units If you get a warning message about your units not being in the standard unit list, a custom unit list needs to be created and added to the additionalMetadata slot of the EML. First we create a custom unit data frame with columns id (the name of the unit, as it appears in the attribute table), unitType, and parentSI. Each custom unit has a row in the data frame. To determine what the unitType and parentSI are for each unit, it may be helpful to reference the list of custom units. This list can easily be viewed in your R console using these commands: standardUnits &lt;- get_unitList() View(standardUnits$units) The following lines create the standard unit data table, use the function set_unitList to slot it into the EML, and then add it as addionalMetadata to the EML. custom_units &lt;- data.frame(id = c(&#39;horsepower&#39;, &#39;gallonPerMinute&#39;), unitType = c(&#39;power&#39;, &#39;volumetricRate&#39;), parentSI = c(&#39;watt&#39;, &#39;litersPerSecond&#39;)) unitlist &lt;- set_unitList(custom_units) eml@additionalMetadata &lt;- c(as(unitlist, &quot;additionalMetadata&quot;)) 5.2.2 Setting other entities 5.2.2.1 Removing other entities In cases where the data package was submitted originally via the registry, the original EML usually has the data tables described as “other entity” elements in the EML. This information is now redundant since we created data table elements describing these objects. Remove the other entities by replacing the other entity element in the EML with a ListOfotherEntity object that consists of an empty list. eml@dataset@otherEntity &lt;- new(&#39;ListOfotherEntity&#39;, list()) 5.2.2.2 Adding other entities for files that aren’t uploaded yet There are times, however, when it may be necessary to create other entity tables for objects that are not described using a data table. Examples of these can include: R scripts, large NetCDF file directories, or audio/image files. Adding other entities is easy as long as you have paths to the files on the server and PIDs. This workflow is best if you are uploading files to the ADC yourself. First get the list of pathnames for the files you are going to upload. paths &lt;- list.files(&quot;/home/sjclark/EML_learning/&quot;, full.names = TRUE) Then generate PIDs for those files. These will be the data PIDs you use in publish_update, but note that you may have other data PIDs for data objects that are not other entities. pids &lt;- vapply(paths, function(x) { paste0(&quot;urn:uuid:&quot;, uuid::UUIDgenerate()) }, &quot;&quot;) This will guess the format ID from the file extension. These should be checked afterwards and potentially changed. format_ids &lt;- guess_format_id(paths) Finally, a data frame is created with all of that information, and that information is added into the EML using eml_add_entities. entity_df &lt;- data.frame(type = &#39;otherEntity&#39;, path = paths, pid = pids, format_id = format_ids, stringsAsFactors = FALSE) eml &lt;- eml_add_entities(eml, entity_df) 5.2.2.3 Adding other entities for files that are already uploaded If you are trying to describe data objects that are already on the ADC, you can utilize lapply along with get_package to easily write the EML otherEntity elements. In this case, there are data tables and other entity type elements mixed in, so the otherEntity elements (non-csvs) are picked out by hand using indexes. The numbers in the call otherEnts &lt;- pkg$data[c(5,6,9,10)] will change depending on your dataset. pid = &#39;doi:10.18739/A2408F&#39; pkg &lt;- get_package(mn, pid, file_names = T) otherEnts &lt;- pkg$data[c(5,6,9,10)] #select only `otherEntity` PIDs after viewing `pkg$data` contents eml@dataset@otherEntity &lt;- new(&quot;ListOfotherEntity&quot;, pid_to_eml_other_entity(mn, otherEnts)) 5.2.3 Setting coverages Sometimes EML documents may lack coverage information describing the temporal, geographic, or taxonomic coverage of a dataset. This example shows how to create coverage information from scratch, or replace an existing coverage element with an updated one. You can view the current coverage (if it exists) by entering eml@dataset@coverage into the console. Here the coverage, including temporal, taxonomic, and geographic coverages, is defined using set_coverage. coverage &lt;- set_coverage(beginDate = &#39;2012-01-01&#39;, endDate = &#39;2012-01-10&#39;, sci_names = c(&#39;exampleGenus exampleSpecies1&#39;, &#39;exampleGenus ExampleSpecies2&#39;), geographicDescription = &quot;The geographic region covers the lake region near Eagle Mountain.&quot;, west = -154.6192, east = -154.5753, north = 68.3831, south = 68.3619) eml@dataset@coverage &lt;- coverage You can also set multiple geographic (or temporal) coverages. Here is an example of how you might set two geographic coverages. geocov1 &lt;- new(&quot;geographicCoverage&quot;, geographicDescription = &quot;The geographich region covers area 1&quot;, boundingCoordinates = new(&quot;boundingCoordinates&quot;, northBoundingCoordinate = new(&quot;northBoundingCoordinate&quot;, 68), eastBoundingCoordinate = new(&quot;eastBoundingCoordinate&quot;, -154), southBoundingCoordinate = new(&quot;southBoundingCoordinate&quot;, 67), westBoundingCoordinate = new(&quot;westBoundingCoordinate&quot;, -155))) geocov2 &lt;- new(&quot;geographicCoverage&quot;, geographicDescription = &quot;The geographich region covers area 2&quot;, boundingCoordinates = new(&quot;boundingCoordinates&quot;, northBoundingCoordinate = new(&quot;northBoundingCoordinate&quot;, 65), eastBoundingCoordinate = new(&quot;eastBoundingCoordinate&quot;, -155), southBoundingCoordinate = new(&quot;southBoundingCoordinate&quot;, 64), westBoundingCoordinate = new(&quot;westBoundingCoordinate&quot;, -156))) coverage &lt;- set_coverage(beginDate = &#39;2012-01-01&#39;, endDate = &#39;2012-01-10&#39;, sci_names = c(&#39;exampleGenus exampleSpecies1&#39;, &#39;exampleGenus ExampleSpecies2&#39;)) eml@dataset@coverage@geographicCoverage &lt;- c(geocov1, geocov2) 5.2.4 Setting methods The methods tree in the EML section has many different options, visible in the schema. You can create new elements in the methods tree by following the schema and using the “new” command. Remember you can explore possible slots within an element by creating an empty object of the class you are trying to create. For example, method_step &lt;- new('methodStep'), and using autocomplete on method_step@. One very simple, and potentially useful way to add methods to an EML that have no methods at all is adding them via a word document. An example is shown below: methods1 &lt;- set_methods(&#39;methods_doc.docx&#39;) eml@dataset@methods &lt;- methods1 If you want to make minor changes to existing method information that has a lot of nested elements, your best bet may be to edit the EML in a text editor, otherwise there is a risk of accidentally overwriting nested elements with blank object classes, therefore losing method information. 5.2.5 Adding people To add people, with their addresses, you need to add addresses as their own object class, which you then add to the contact, creator, or associated party classes. NCEASadd &lt;- new(&#39;address&#39;, deliveryPoint = &#39;735 State St #300&#39;, city = &#39;Santa Barbara&#39;, administrativeArea = &#39;CA&#39;, postalCode = &#39;93101&#39;) The creator, contact, and associated party classes can easily be created using functions from the arcticdatautils package. Here, we use eml_creator to set our dataset creator. JC_creator &lt;- eml_creator(&quot;Jeanette&quot;, &quot;Clark&quot;, &quot;NCEAS&quot;, &quot;jclark@nceas.ucsb.edu&quot;, phone = &#39;123-456-7890&#39;, userId = &#39;https://orcid.org/WWWW-XXXX-YYYY-ZZZZ&#39;, address = NCEASadd) eml@dataset@creator &lt;- c(JC_creator) Similarly, we can set the contacts. In this case, there are two, so we set eml@dataset@contact as a ListOfcontact, which contains both of them. JC_contact &lt;- eml_contact(&quot;Jeanette&quot;, &quot;Clark&quot;, &quot;NCEAS&quot;, &quot;jclark@nceas.ucsb.edu&quot;, phone = &#39;123-456-7890&#39;, userId = &#39;https://orcid.org/WWWW-XXXX-YYYY-ZZZZ&#39;, address = NCEASadd) JG_contact &lt;- eml_contact(&quot;Jesse&quot;, &quot;Goldstein&quot;, &quot;NCEAS&quot;, &quot;jclark@nceas.ucsb.edu&quot;, phone = &#39;123-456-7890&#39;, userId = &#39;https://orcid.org/WWWW-XXXX-YYYY-ZZZZ&#39;, address = NCEASadd) eml@dataset@contact &lt;- c(JC_contact, JG_contact) Finally, the associated parties are set. Note that associated parties MUST have a role defined, unlike creator or contact. JG_ap &lt;- eml_associated_party(&quot;Jesse&quot;, &quot;Goldstein&quot;, &quot;NCEAS&quot;, &quot;jclark@nceas.ucsb.edu&quot;, phone = &#39;123-456-7890&#39;, address = NCEASadd, userId = &#39;https://orcid.org/WWWW-XXXX-YYYY-ZZZZ&#39;, role = &quot;metaataProvider&quot;) eml@dataset@associatedParty &lt;- c(JG_ap) 5.3 Validating and writing the EML The last step is to write and validate the EML. For time-saving purposes, first you write the EML: path2 &lt;- '/path/to/new/eml/Lake Physical Properties Data.xml' write_eml(eml, path2) Now, you validate the eml. The EML validation step indicates whether the EML that you’ve created is valid both with regard to EML and XML schema. Hopefully it returns TRUE. eml_validate(path2) If the EML validate returns FALSE, it is accompanied by an error that will be in this format: 69.0: Element 'boundingCoordinates': This element is not expected. Expected is one of ( geographicDescription, references ). This error essentially says, the EML validator reached the slot boundingCoordinates but did not expect this element. Instead it expected either geographicDescription or references. Referring to the schema maps (eg: https://knb.ecoinformatics.org/emlparser/docs/eml-2.1.1/eml-coverage.png) you can see that before bounding coordinates, there must be a geographic description. The fix would be to return to your definiton of the geographicCoverage, and insert a geographicDescription into the geographicCoverage object (ie:geocov1 &lt;- new('geographicCoverage', geographicDescription = 'Description here',...)). 5.4 Dealing with unusual cases Typically, the biggest issue most datateam members have using the EML package is trying to add multiple values within a slot. In some cases, like adding multiple dataTable instances or multiple creator instances, this is easy, as is shown above. Other times, particularly if the slot is nested within a slot that we use a helper function for (like set_attributes), it can get a little more challenging. By delving a little deeper into the EML S4 class of objects though, you can resolve most problems relatively easily. Any slot that allows for a list of objects, as listed in the EML schema (https://knb.ecoinformatics.org/#external//emlparser/docs/eml-2.1.1/index.html), will have a class called the slot name, in addition to a class called ListOfslotName. You can use the new function to create an object of the slot itself and a list of objects within that slot. For example: m1 &lt;- new(&#39;missingValueCode&#39;, code = &#39;NA&#39;, codeExplanation = &#39;No data taken&#39;) m2 &lt;- new(&#39;missingValueCode&#39;, code = &#39;-999&#39;, codeExplanation = &#39;Sensor malfunctioned&#39;) codes &lt;- new(&#39;ListOfmissingValueCode&#39;, list(m1, m2)) In this case, this slot is nested within a part of the EML that we typically construct with a helper function - so how do we actually get this list of missing value codes into the attribute table? One strategy would be to construct the attribute list without the missing value code information for the attribute with mutliple attributes, and then insert the ListOfmissingValueCode into it. So, building our attribute list as normal, attributes1 &lt;- read.csv(&#39;~/arctic-data/docs/training/EML/LakeSampleData.csv&#39;, stringsAsFactors = F) attlist1 &lt;- set_attributes(attrbutes1) You can then dig down into the individual attributes using the @ functionality. Note that all of the attributes are stored as a ListOfattributes object. You can view a single value within this list by calling attlist1@attributes@.Data[[n]] where n represents the index of the list you want to view. attlist1@attribute@.Data[[1]] &lt;attribute system=&quot;uuid&quot;&gt; &lt;attributeName&gt;Date&lt;/attributeName&gt; &lt;attributeDefinition&gt;Date sample was taken on&lt;/attributeDefinition&gt; &lt;measurementScale&gt; &lt;dateTime&gt; &lt;formatString&gt;MM-DD-YYYY&lt;/formatString&gt; &lt;/dateTime&gt; &lt;/measurementScale&gt; &lt;/attribute&gt; You can also run commands on attlist1@attribute@.Data just like you would any other list. One useful one is length(attlist1@attribute@.Data) which will return the number of items in the list. So to insert the list of missing value codes, you’ll need to navigate to the index in the list of the attribute that you need to modify, and then dig even deeper into the slots. Don’t forget that the tab key is your friend and can help you find slot names! So if you know that you need to change the 6th attribute in the list, you can type the following to see what is in there now: attlist1@attribute@.Data[[6]]@missingValueCode An object of class &quot;ListOfmissingValueCode&quot; [[1]] &lt;missingValueCode/&gt; Now to add the list, you’ll have to execute the following (repeating lines for clarity): m1 &lt;- new(&#39;missingValueCode&#39;, code = &#39;NA&#39;, codeExplanation = &#39;No data taken&#39;) m2 &lt;- new(&#39;missingValueCode&#39;, code = &#39;-999&#39;, codeExplanation = &#39;Sensor malfunctioned&#39;) codes &lt;- new(&#39;ListOfmissingValueCode&#39;, list(m1, m2)) attlist1@attribute@.Data[[6]]@missingValueCode &lt;- codes Now, if you view this attribute again you should see the mutliple missing value codes: &gt; attlist1@attribute@.Data[[6]]@missingValueCode An object of class &quot;ListOfmissingValueCode&quot; [[1]] &lt;missingValueCode&gt; &lt;code&gt;NA&lt;/code&gt; &lt;codeExplanation&gt;No data taken&lt;/codeExplanation&gt; &lt;/missingValueCode&gt; [[2]] &lt;missingValueCode&gt; &lt;code&gt;-999&lt;/code&gt; &lt;codeExplanation&gt;Sensor malfunctioned&lt;/codeExplanation&gt; &lt;/missingValueCode&gt; What I showed above is a pretty specific example (which conveniently is also one of our more common “rare cases”), but there are some general strategies you can employ to figure out how to insert other objects that need to be deeply nested into an EML. Create an empty test object: example: test &lt;- new('abstract') Use the @ and tab functionality to explore slots in that object: example: test@para What are all these slots??: Remember that the schemaLocation, lang, slot_order, and section slots are related to the schema and are not actual slots which contain values you can edit. .Data: if you run into a .Data using the tab key, that means that the slot can be part of a list. Use indexing to access individual items in a list: example: test@para@.Data[[2]] Index out of bounds: If you receive an “index out of bounds” error that means you are trying to access an index beyond what exists in the list. For example, if there are 4 paragraphs in the abstract, and you write test@para@.Data[[5]] you will recieve this error. Use the schema!: https://knb.ecoinformatics.org/#external//emlparser/docs/eml-2.1.1/index.html 5.5 Exercise Read in the EML that you updated in the previous exercise into R Use the eml package to replace the exsiting dataTable slot with a new dataTable object with an attribute list and physical section you wrote in R Write and validate your EML Update your package with the new EML using publish_update Use the checklist Make edits where necessary, and publish an update if needed "],
["using-git-in-rstudio.html", "Chapter 6 Using git in RStudio 6.1 Introduction 6.2 Setting up git 6.3 Working with the repository 6.4 My Git tab disappeared", " Chapter 6 Using git in RStudio 6.1 Introduction First, read the excellent intro to git put together by the eco-data-science group at UCSB/NCEAS. Don’t worry too much about the forking and branching sections, as we will primarily be using the basic commit-pull-push commands. git intro 6.1.1 So why do I need to use this again? There are several reasons why using the arctic-data github is helpful, both for you and for the rest of the data team. Here are a few: Versioning: Accidentally make a change to your code and can’t figure out why it broke? Wish you could go back to that version that worked? If you add your code to the github you can do this! Reproducibility: Being able to reproduce how you acomplished something is incredibly important. We should be able to tell anyone exactly how data have been reformatted, how metadata have been altered, and how packages have been created. This is especially important for us, as a data center, with interns that stay for 6-12 months, if we need to go back and figure out how something was done after the intern who wrote the code left. Troubleshooting: If you are building a particularly complicated EML, or doing some other advanced task, it is much easier for Jesse, Jeanette, or Bryce to troubleshoot your code if it is on the github. We can view, troubleshoot, and fix bugs very easily when code is on the github, with the added bonus of being able to go back a version if something should break. Solve future problems: Some of the issues we see in ADC submissions come up over and over again. When all of our code is on github, we can easily reference code built for other submissions, instead of trying to solve the same problems over and over again from scratch. 6.2 Setting up git Now you need to set up your git global options, and tell it who you are. At the top of your RStudio window, select Tools &gt; Shell. In the prompt, you will need to run two commands, one at a time. The first tells git what your name is, the second what your email address is. These are the commands: git config --global user.name &quot;My Name&quot; git config --global user.email myemail@domain.com After running these commands, the shell prompt should look like this: 6.2.1 Cloning the arctic-data repo Next, you need to clone the arctic-data repository to your RStudio. You do this by adding it as a “project.” In your RStudio window, click File &gt; New Project. Then click Version Control, and then select the Git option. If you are prompted to save your workspace during this process, make sure all of your work is saved, and you don’t need anything in your environment, and then click ‘Don’t Save.’ You should see a prompt asking you for a URL. Fill it out like this to clone the arctic-data repository into the top level of your home directory. Note that the URL is the same URL you use to view the repository on the web. If you are using the sasap-data repository, the URL is http://github.nceas.ucsb.edu/NCEAS/sasap-data/. clone repo You will be prompted for your username and password, and then git will clone the directory. The username/password you use should be the same one you use to log in when you go to http://github.nceas.ucsb.edu/KNB/arctic-data. Now you should have a directory called arctic-data in your RStudio files window. 6.3 Working with the repository 6.3.1 Adding a new script If you have been working on a script that you want to put in the arctic-data github, you need to save it somewhere in the arctic-data folder you made in your account on the server. You can do this by either moving your script into the folder or using the save-as functionality. Note that git will try and version anything that you save in this folder, so you should be careful about what you save here. Things that probably shouldn’t be saved in this folder, for our purposes include: Tokens: Any token file or script with a token in it should NOT be saved in the repository. Others could steal your login credentials if you put a token on the github. Data files: Git does not version data files very well. You shouldn’t save any .csv’s or any other data files (including metadata) Workspaces/.RData: If you are in the habit of saving your R workspace, you shouldn’t save it in this directory Plots/Graphics: For the same reasons as data files Note: Do not EVER make a commit that you don’t understand. If something unexpected (like a file you have never worked on) shows up in your git tab, ask Jesse or Jeanette before committing. After you save your script in the appropriate place within the arctic-data folder, it will show up in your git tab looking like this: Before you commit your changes, you need to click the little box under “staged.” Do not stage or commit any .Rproj file. After clicking the box for your file, click “Commit” to commit your changes. In the window that pops up (you may need to force the browser to allow pop-ups), you can write your commit message. Remember that the commit message should be a concise description of the changes being made to a file. Your window should look like this: Push commit, and your commit will be saved. Now you want to merge the commits you made with the master version of the repository. You do this by using the command “push.” Before you push, you need to pull though, always, to avoid merge conflicts. Click “pull” and type in your credentials. Then, assuming you don’t have a merge conflict, you can push your changes by clicking “push.” Always remember, the order is commit-pull-push 6.3.2 Editing a script If you want to change a script, the workflow is the same. Just open the script that was saved in the arctic-data folder on your server account, make your changes, save the changes, stage them by clicking the box, commit, pull, then push to merge your version with the main version on the website. Do NOT edit scripts using the website. It is much easier to accidentally overwrite the history of a file this way. One thing you might be wondering as you are working on a script is, how often should I be committing my changes? It might not make sense to commit-pull-push after every single tiny change - if only because it would slow you way down. Personally, I commit every time I feel that a significant change has happened and that the chunk of code I was working on is “done.” Sometimes this is an entire script, other times it is just a few lines within a script. A good sign that you are committing too infrequently might be if many of your commit messages address a wide variety of coding tasks, such as: “wrote for loop to create referenced attribute lists for tables 1:20. also created nesting structure for this package with another package. also created attribute list for data table 40.” And one final note, you can make multiple commits before you push to the repo. If you are making lots of changes to the script, you might want to make several commits before pull-push. You can see how many commits you are ahead of the “origin/master” branch (i.e. what you see on the website) by looking for text in your git tab in RStudio that looks like this: 6.3.3 Where do I commit? The default right now is to save data-processing scripts in the arctic-data/datateam/data-processing/ directory, with subfolders listed by project. Directories can be created as needed but please ask Jesse or Jeanette first so we can try and maintain some semblance of order in the file structure. 6.4 My Git tab disappeared Sometimes R will crash so hard it loses your project information, causing your git tab to disappear. If this happens anything you saved, but did not commit or push in your arctic-data (or sasap-data) folder is no longer being tracked by github. To get the tab back, first, rename your old arctic-data folder to something else (like arctic-data_old). This will ensure that any work that you had in that folder that you didn’t push to the master branch is not lost. Next, follow the steps above in “cloning the arctic-data repo” to re-clone the repository. Then, move whatever scripts or parts of scripts that were not being tracked into the arctic-data repo so that they are tracked again, and merge them back into the master branch. Ideally, you are committing and pushing your scripts frequently enough that you don’t have to resort to this. If you had changes you had committed but not pushed, you can still push these changes from the command line. See Jeanette for more info on how to do this if it is the case. Remember: commit, pull, push frequently (at least once a day). "],
["introduction-to-solr.html", "Chapter 7 Introduction to Solr 7.1 Querying Solr 7.2 Querying Solr through R 7.3 Key takeaways 7.4 More resources", " Chapter 7 Introduction to Solr Solr is what’s known as an index. More specifically, it’s a piece of software that we install with every instance of Metacat which we use to query all of the Objects Metacat stores (metadata, data, resource maps, etc.). Every Object in Metacat will have a corresponding Solr document for it that contains information about that Object. Each type of Object will have a different set of fields in its Solr document. For example, an EML document will have a title field (corresponding to the EML document’s &lt;title&gt; element), while a CSV will not. The fields that go into a Solr document are populated by information such as: The System Metadata (fileName, accessPolicy, etc) The Object itself (e.g. title, creators, etc. for an EML record) Additional computed fields (e.g., a geohash for quick spatial search) Indexing the Objects Metacat stores lets us execute some interesting and very useful queries such as: What are the most recently updated datasets? What Metadata and Data Objects are in a given Data Package? What is the total size (in terms of disk space) of all Objects stored in Metacat? 7.1 Querying Solr Solr is queried via what’s called an HTTP API. Practically, what this means it that you can visit a URL (web address) in a web browser to execute a query. This may be a little bit weird at first but I hope some examples will make it more clear. So I said you visit a URL to query Solr. But what address do you visit? For the Arctic Data Center (https://arcticdata.io), every Solr query starts with a base URL of https://arcticdata.io/metacat/d1/mn/v2/query/solr/. If you visit that URL, you will see a list of fields Solr is storing for the Objects it indexes: &lt;ns2:queryEngineDescription xmlns:ns2=&quot;http://ns.dataone.org/service/types/v1.1&quot;&gt; &lt;queryEngineVersion&gt;3.6.2.2012.12.18.19.52.59&lt;/queryEngineVersion&gt; &lt;name&gt;solr&lt;/name&gt; &lt;queryField&gt; &lt;name&gt;abstract&lt;/name&gt; &lt;description&gt; The full text of the abstract as provided in the science metadata document. &lt;/description&gt; ...truncated... You can see that there is a large set of queryable fields, though, as I said above, not all types of Objects will have values set for all of the possible fields because some fields do not make sense for some Objects (e.g., title for a CSV). 7.1.1 Parts of a Query Each Solr query is comprised of a number of parameters. These are like arguments to a function in R, but they are entered as parts of a URL. The most common parameters are: q: The query. This is like subset or dplyr::filter in R fl: What fields are returned for the documents that match your query (q). If not set, all fields are returned. rows: The maximum number of documents to return. Solr will truncate your result if the result size is greater than rows. sort: Sorts the result by the values in the given Solr field (e.g., sort by date uploaded) To use these parameters, we append to the base URL like this: https://arcticdata.io/metacat/d1/mn/v2/query/solr/?q={QUERY}&amp;fl={FIELDS}&amp;rows={ROWS} and we replace the text inside {} with the value we want for each parameter. Note that the parameters can come in any order so the following is equivalent: https://arcticdata.io/metacat/d1/mn/v2/query/solr/?fl={FIELDS}&amp;rows={ROWS}&amp;q={QUERY} The first parameter in the URL must have a ‘?’ in front of it and all subsequent parameters must have an ‘&amp;’ between them. It’s really easy to get the URL wrong when typing it in manually like this so be sure to double-check your URL and think critically about the result: Solr tries to always return something even if it’s not what you intended. 7.1.2 Constructing a Query The query (‘q’) parameter uses a syntax that looks like field:value, where field is one of the Solr fields and value is an expression. The expression can match a specific value exactly, e.g., https://arcticdata.io/metacat/d1/mn/v2/query/solr/?q=identifier:arctic-data.7747.1 https://arcticdata.io/metacat/d1/mn/v2/query/solr/?q=identifier:&quot;doi:10.5065/D60P0X4S&quot; which finds the Solr document for a specific Object by PID (identifier). Note that in the second example, the DOI PID is surrounded in double quotes. This is because Solr has reserved characters, of which ‘:’ is one, so we have to help Solr by surrounding values with reserved characters in them in quotes (as I did here) or escaping them. Queries can take on a more advanced form such as a wildcard expression: https://arcticdata.io/metacat/d1/mn/v2/query/solr/?q=identifier:arctic-data.* finds all the Objects that start with “arctic-data.” followed by anything (“*“) https://arcticdata.io/metacat/d1/mn/v2/query/solr/?q=title:*soil* finds all the Objects with the word “soil” somewhere in the title. https://arcticdata.io/metacat/d1/mn/v2/query/solr/?q=origin:*Stafford*+AND+title:Bering Strait&amp;fl=title&amp;sort=title+desc&amp;rows=10 finds 10 Objects where one of the EML creators has a name that contains the substring “Stafford” and the title contains the substring “Bering Strait”, sorted by title (descending order). Note that the +AND+ between the origin and title query above specifies that both conditions must be true for a Solr document to be returned. We could’ve also switched the +AND+ to +OR+ and/or added more conditions to the query. Here’s a slightly more advanced one: https://arcticdata.io/metacat/d1/mn/v2/query/solr/?q=formatType:METADATA+AND+-obsoletedBy:*&amp;sort=dateUploaded+desc&amp;rows=25 This query is the query MetacatUI uses to fill in the https://arcticdata.io/catalog/ page. Notice the -obsoletedBy:*. The ‘-’ before the field inverts the expression so this part of the query means “things that have no obsoletedBy value set”. We can also just find everything: https://arcticdata.io/metacat/d1/mn/v2/query/solr/?q=*:* finds any value in any field. 7.1.3 Faceting Above we went through querying across Solr documents but we can also summarize what’s in Solr with Faceting which lets us group Solr documents together and count them. This is like table in R. Faceting can do a query within a query, but more commonly I use it to summarize unique values in a field. For example, we can find the unique format IDs on Data Objects: https://arcticdata.io/metacat/d1/mn/v2/query/solr/?q=*:*&amp;fq=formatType:DATA&amp;facet=true&amp;facet.field=formatId&amp;rows=0 To facet, we usually do a few things: Add the parameter facet=true Add the parameter facet.field={FIELD} with the field we want to facet (group) on Set rows=0 because we don’t care about the matched Solr documents Optionally specify fq={expression} which filters out Solr documents before faceting. In the above example, we have to do this to only count Data Objects. Without it, the facet result would include formatIDs for metadata and resource maps which we don’t want. 7.1.4 Stats With Faceting, we found we could make queries to find the unique values for a Solr field. With Stats, we can have Solr calculate statistics on numerical values (such as fileSize). https://arcticdata.io/metacat/d1/mn/v2/query/solr/?q=formatType:DATA&amp;stats=true&amp;stats.field=size&amp;rows=0 This query calculates a set of summary statistics for the size field on Data Objects that Solr has indexed. In this case, Solr’s size field indexes the fileSize field in the System Metadata for each Object in Metacat. 7.2 Querying Solr through R What if I told you that every time you run the query function in the dataone R package you are asking R to visit a URL like the ones above, parse the information returned by the page, and present it in an R-friendly way such as a list or data.frame? Well that’s what happens! Then why might we use R in the first place? There are two big advantages: The result is returned in a more useful way to R without extra work on your part We can more easily pass our authentication token with the query Why does #2 matter? Well by default, all of those URLs above only returned publicly-readable Solr documents. If a private document matched any of those queries, Solr doesn’t give you any idea and acts like the non-public-readable documents don’t exist. So we must pass an authentication token to access non-public-readable content. This bit is crucial for working with the Arctic Data Center so you’ll very often want to use R instead of visiting those URLs in a web browser. And there’s good news: all of the URLs you visited above can be turned into an R expression very easily. For example: https://arcticdata.io/metacat/d1/mn/v2/query/solr/?q=title:*soil* becomes library(dataone) cn &lt;- CNode(&quot;PROD&quot;) mn &lt;- getMNode(cn, &quot;urn:node:ARCTIC&quot;) # Set your token if you need/want! query(mn, &quot;q=title:*soil*&amp;fl=title&amp;rows=10&quot;) I just deleted the first part of the URL, up to and including the ‘?’, and pasted the rest in as the second argument to query. You may have seen an alternative syntax: query(mn, list(q=&quot;title:*soil*&quot;, fl=&quot;title&quot;, rows=&quot;10&quot;)) this is the same query as above because the query function takes either a string (the first form) or a named list (the second form). By default, query returns the result as a list. This is definitely useful but a data.frame can be a more useful way to work with the result. To get a data.frame instead, just set the as argument to ‘data.frame’ to get a data.frame: query(mn, list(q=&quot;title:*soil*&quot;, fl=&quot;title&quot;, rows=&quot;10&quot;), as = &quot;data.frame&quot;) 1 Daily Average Soil, Air and Ground Temperatures - Council Forest Site [Romanovsky, V.] 2 Canadian Transect of Soils and Vegetation for the Circumpolar Arctic Vegetation Map 3 Soil Temperatures, Toolik Lake, Alaska, 1995 and 1996 4 Soil Temperature ARCSS grid Atqasuk, Alaska 2013 5 X-ray fluorescence, Barrow soils 6 Toolik Lake, Alaska Soil moisture - 2014 7 Thermal Soil Properties for Ivotuk and Council [Beringer, J.] 8 Soil Temperature NIMS grid Barrow, Alaska 2014 9 Ivotuk Soil Data - Station Met2 [Hinzman, L.] 10 thule_tram_soil_temps_2013.csv 7.3 Key takeaways Find out what you can query at https://arcticdata.io/metacat/d1/mn/v2/query/solr The Solr HTTP API is what the R dataone package uses when you run query Pass a token if you want to include non-public-readable objects in the results (you often do!) 7.4 More resources Solr’s The Standard Query Parser docs (high level of detail) Another quick reference: https://wiki.apache.org/solr/SolrQuerySyntax http://www.solrtutorial.com/ "]
]

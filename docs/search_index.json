[
["index.html", "NCEAS Data Team Training Chapter 1 Welcome to NCEAS! 1.1 First day to-dos: 1.2 Account information 1.3 NCEAS Events", " NCEAS Data Team Training Jeanette Clark 2017-12-14 Chapter 1 Welcome to NCEAS! 1.1 First day to-dos: Get a tour of the office from Ginger, Jeanette, or Jesse Fill out required paperwork from Michelle Have Ana take your picture (room 309) Set up the remainder of your accounts 1.2 Account information LDAP - NCEAS - Jeanette or Jesse should have set this up prior to your start date to help get other accounts set up by the first day. This account and password control your access to: arcticdata RT queue Github - arctic-data and sasap-data Datateam server - follow instructions in email from Nick at NCEAS to reset your datateam password in the terminal ORCiD - create an account NCEAS Slack - get an invite from slack.nceas.ucsb.edu Arctic Data Center Team - after creation of ORCiD and sign-in to both arcticdata.io and test.arcticdata.io, request an add from Chris on Slack Schedule - fill out anticipated quarterly schedule 1.3 NCEAS Events NCEAS hosts a number of events that you are encouraged to attend. Keep an eye on your email but the weekly events are : Roundtable weekly presentation and discussion of research by a visiting or local scientist Wednesdays at 12:15 in the lounge Coffee Klatch coffee, socializing, and news updates for NCEAS Tuesdays at 10:30 in the lounge Salad Potluck potluck salad and socializing, bring a topping or side to share! second Tuesday of the month, 12:15 in the lounge "],
["introduction-to-open-science.html", "Chapter 2 Introduction to open science 2.1 Open science background reading 2.2 Effective data management 2.3 Using DataONE 2.4 Creating metadata 2.5 Working on a remote server", " Chapter 2 Introduction to open science These materials are meant to introduce you to the principles of open science, effective data management, and data archival with the DataONE data repository. 2.1 Open science background reading Read the content on the Arctic Data Center (ADC) webpage to learn more about data submission, preservation, and the history of the ADC about submission preservation history 2.2 Effective data management Read Matt Jones’ paper on effective data management to learn how we will be organizing datasets prior to archival. 2.3 Using DataONE The Data Observation Network for Earth (DataONE) is a community driven project providing access to data across multiple member repositories, supporting enhanced search and discovery of Earth and environmental data. Watch the first 38 minutes of this video explaining how DataONE works. This video is pretty technical, and you may not understand it all at first. Please feel free to ask Jesse or Jeanette questions. 2.4 Creating metadata Most submitters to the ADC submit their data and metadata through an online form we call the registry. This is how you will create your first metadata record. The metadata specification we use is called the Ecological Metadata Language, or EML. 2.4.1 Exercise Use the “tabula” program to extract Table 1 from this paper Reformat the table using R under the guidelines in the journal article on effective data management If you need an R refresher, take as much time as you need to go over the data carpentry guide You may also find the data carpentry lesson on dplyr helpful Go to test.arcticdata.io and submit your reformatted file with appropriate metadata that you derive from the text of the paper list yourself as the first creator so your test submission can easily be found for the purposes of this training exercise, not every single author needs to be listed with full contact details, listing the first two authors is fine attributes (column names) should be defined, including correct units and missing value codes 2.5 Working on a remote server All of the work that we do at NCEAS is done on our remote server, called “datateam.” If you have never worked on a remote server before, you can think of it like working on a different computer via the internet. We access RStudio for our server through the link datateam.nceas.ucsb.edu/rstudio/. To transfer files on and off of the server, you’ll need to use either bash commands in the terminal, or a program called cyberduck. 2.5.1 Cyberduck instructions To use cyberduck to transfer local files onto the datateam server: Open Cyberduck Click “Open Connection” From the drop-down, choose “SFTP (SSH File Transfer Protocol)” Enter “datateam.nceas.ucsb.edu” for Server Enter your username and password Connect From here, you can drag and drop files onto and off of the server. 2.5.2 A note on paths On the servers, paths to files in your folder always start with /home/yourusername/.... When you write scripts, try to avoid writing relative paths (which rely on what you have set your working directory to) as much as possible. Instead, write out the entire path as shown above, so that if another intern or Jesse or Jeanette need to run your script, it is not dependent on a working directory. arcticdatautils * Compare your submission against the checklist and update it until it meets all the checks by: + Updating data files as necessary, + Manipulating the EML using the EML package, + and then updating the submission on the test site using arcticdatautils. * Finally, upload your script to the GitHub Enterprise repository by following these instructions "],
["using-arcticdatautils.html", "Chapter 3 Using arcticdatautils 3.1 Uploading packages using R 3.2 Uploading a new version", " Chapter 3 Using arcticdatautils 3.1 Uploading packages using R We will be using the arcticdatautils package and the dataone package to connect to the NSF Arctic Data Center data repository and push and pull edits. To identify yourself as admin you will need to register a ‘token’ by signing in to the ADC with your ORCiD, copying your token into R and telling R where to connect by defining the member node using the code below. library(arcticdatautils) library(dataone) 3.1.1 Set environment Once we have our libraries loaded we need to tell R which repo we want to be working with. If we are working on the production site, you can set the coordinating node (cn) and member node (mn) this way: cn &lt;- CNode(&#39;PROD&#39;) mn &lt;- getMNode(cn,&#39;urn:node:ARCTIC&#39;) If we are working in the test environment, we set it this way: cn &lt;- CNode(&#39;STAGING&#39;) mn &lt;- getMNode(cn,&#39;urn:node:mnTestARCTIC&#39;) A note on nodes - be very careful about what you publish on the production node (PROD, or arcticdata.io). This node should NEVER be used to publish test or training datasets. While going through training exercises, you should be using the test environment (STAGING, or test.articdata.io). 3.1.2 Set token Now that we have the environment set we have to manually log in to the NSF ADC site to get our token and copy it to our R environment. This token is your identity on these sites, please treat it as you would a password (ie. don’t paste into scripts that will be shared). The easiest way to do this is to just run the token in the console. There’s no need to keep it in you script since its temporary anyway. Sometimes you’ll see a placeholder in scripts to remind users to get their token, such as: options(dataone_test_token = &quot;...&quot;) 3.1.3 What is in a package? A data package generally consists of at least 3 “objects” or files. One object is the metadata file itself. Ths file is in XML format, and has the extension .xml. We sometimes refer to this file as the EML, which is the metadata standard that it uses. Another object or objects are the data files themselves. Most commonly these are data tables (.csv), but they can also be audio files, netCDF files, plain text files, pdf documents, image files, etc. The final object is the resource map. This object is a text file that defines the relationships between all of the other objects in the data package. It says things like “this metadata file describes this data file,” and is critical to making a data package render correctly on the website with the metadata files and all of the data files in te right place. Fortunately, we rarely if ever have to actually look at the contents of resource maps, they are generated for us using arcticdatautils. 3.1.4 About identifiers Each object (metadata files, data files, resource maps) on the Artic Data Center or KNB has a unique identifier, also sometimes called a “pid”. When you look at the landing page for a dataset, for example here, you can find the resource map identifier in the URL (resource_map_doi:10.18739/A2836Z), the metadata identifer in the “General &gt; Identifier” section of the metadata record (doi:10.18739/A2836Z), and the data identifier by clicking the “more info” link next to the data object, and looking at the “online distribution info” section (arctic-data.9546.1). Different versions of a package are linked together by what we call the version chain. Making an update to a data package, such as replacing a data file, changing a metadata record, etc, will result in a new identifier for the object that was updated. When making changes to a package, always use the update_object or publish_update commands from arcticdatautils to ensure that the version chain is maintained. 3.1.5 Publishing a package to the site Objects are published to the site using a function called publish_object. To publish a metadata file you would use the following call: meta_path &lt;- &#39;path/to/your/metadata/metadata.xml&#39; pid &lt;- publish_object(mn, meta_path, format_id = format_eml()) This call will save the id of the metadata object to the variable pid, which you will need later for the resource map. It also adds a formatID, which identifies what kind of object you just uploaded. Similarly, you can publish data objects like this: data_path &lt;- &#39;path/to/your/data/data.csv&#39; data_pid &lt;- publish_object(mn, data_path, format_id = &#39;text/csv&#39;) Note that you will need to be explicit about your format_id here based on the file type. A list of format IDs can be found here on the DataONE website. Finally, you can create and publish the resource map using create_resource_map. rm &lt;- create_resource_map(mn, pid, data_pids = data_pid) One final step is to make sure that the rights and access on thes objects you uploaded are set correctly. The function set_rights_and_access will set both, and set_access will just set access. There are two functions for this because a rights holder should always have access, but not all people who need access are rights holders. The rights holder to the dataset is typically the submitter (if the dataset is submitted through the registry), but if a data team intern is publishing objects for a PI, the rights holder is the main point of contact for the dataset (ie the person who requested that we upload the data for them). In this example, we set rights and access for all of the objects created above, using an example ORCID. set_rights_and_access(mn, c(pid, data_pid, rm), subject = &#39;http://orcid.org/PUT0-YOUR-ORCD-HERE&#39;, permissions = c(&#39;read&#39;, &#39;write&#39;, &#39;changePermission&#39;)) 3.1.6 Exercise Locate the data package you published by navigating to the “my profile” section on test.arcticdata.io. Download the metadata and data files and transfer them to the datateam server. Using the functions described in the section above, pubish your metadata record and data file to the site. View your new dataset (which is identical to the one you created previously) by appending the pid of your resource map (in the example above, the resource map pid was saved to the variable rm) to the end of the URL test.arcticdata.io/#view/… 3.2 Uploading a new version To edit material, we use the publish_update or update_object functions in arcticdatautils. Usually we’ll be updating exisiting work using publish_update() from the arcticdatautils package, which has an argument for adding data PIDs (or otherwise including existing data PIDs to make sure that they stay with the package). This function allows you to add or remove data objects, and/or make metadata edits. The following example shows how you would add a data file to a package, and make a change to the metadata file. First, make whatever edits you ned to make to the metadata, and save them to the server. Define the path to your metadata. meta_path &lt;- &quot;path/to/metadata/file.xml&quot; Use either the metadata or resource map pid for your existing data package and the function get_package to return all of the existing pids in the data package as a named list. # the function get_package is helpful to get all other PIDs in a package if you know the metadata PID metaPid &lt;- &quot;urn:uuid:5933a052-5493-4f50-8622-ed83231d3fee&quot; ids &lt;- get_package(mn, metaPid) Finally, use the publish_update function with the results of the lines above to update your data package. publish_update(mn, metadata_pid = metaPid # old metadata PID you pulled in originally. Alternatively could paste in the string: &quot;urn:uuid:a592a9a2-547c-48ee-bff2-add133aa64ee&quot; resource_map_pid = ids$resource_map, metadata_path = meta_path, # path to updated EML file, data_pid = ids$data, # this can also be 2+ by including the PIDs in a vector c(pid1, pid2,pid3) check_first = T, use_doi = FALSE, public = FALSE) If a package is ready to be public, note that you can change the public argument in the publish_update call to TRUE. Similarly, if you want to publish with a DOI type identifier instead of a UUID type identifer, you can change the use_doi argument to TRUE. 3.2.1 Exercise Locate the data package you published in the previous exercise by navigating to the URL test.arcticdata.io/#view/… with the resource map pid you generated in that exercise. Download the EML to your computer, and open it in a plain text editor (TextEdit or Atom). Navigate to the title section, and make a change to the title text within the &lt;title&gt;…&lt;/title&gt; section. Do not change the &lt;title&gt; or &lt;/title&gt; syntax. Upload the file back to the datateam server. Using the publish_update function, update your data package as shown above. "],
["system-metadata.html", "Chapter 4 System metadata 4.1 Introduction 4.2 Editing sysmeta 4.3 Editing sysmeta on lots of files 4.4 Identifiers and sysmeta", " Chapter 4 System metadata 4.1 Introduction Every object in the Arctic Data Center (or on the KNB) has “system metadata.” An object’s system metadata has information about the file itself, such as the name of the file, the format, who the rights holder is, and what the access policy is (amongst other things). Sometimes we will need to edit system metadata in order to make sure that things on the webpage display correctly, or to ensure a file downloads from the website with the correct file name and extension. Although the majority of system metadata changes that need to be made are done with the functions in arcticdatautils, sometimes we need to change aspects of the system metadata (or sysmeta) outside of those functions. This markdown explains how to do this using the functions getSystemMetadata and updateSystemMetadata. 4.2 Editing sysmeta First we need to load in some packages, and set up our environment. Note that typically if you are editing sysmeta you will need to set a token just like if you were updating a package. library(arcticdatautils) library(dataone) cn &lt;- CNode(&#39;STAGING&#39;) mn &lt;- getMNode(cn,&#39;urn:node:mnTestARCTIC&#39;) #Loading in the test environment for this example Next, we input the mn instance and PID of interest into the getSystemMetadata function. Note that the PID should be of the object of interest, not just the package of interest. Each individual object (metadata file, resource map, data file) will have its own system metadata. pid &lt;- &#39;urn:uuid:9a1b02a8-713e-4682-aafb-95c854a4c24a&#39; sysmeta &lt;- getSystemMetadata(mn, pid) This returns an S4 object (more on these objects in Chapter 4), called sysmeta. The sysmeta object has the following “slots”: * serial version * identifier * formatId * checksum * checksumAlgorithm * submitter * rightsHolder * accesPolicy * replicationAllowed * numberReplicas * preferredNodes * blockedNodes * obsoletes * obsoletedBy * archived * dateUploaded * dateSysMetaModified. You can view and edit slots using the @ functionality. sysmeta@fileName [1] &quot;WELTS_flatfile.csv&quot; If you want to change a slot, you can simply do the following: sysmeta@fileName &lt;- &#39;AlaskaWells.csv&#39; Note that some slots cannot be changed by simple text replace (particularly the accessPolicy). There are various helper functions for changing the accessPolicy and rightsHolder such as addAccessRule (which takes the sysmeta as an input) or set_access, which only requires a PID. In general, you most frequently need to use getSystemMetadata to change either the formatId or fileName slots. After you have changed the necessary slot, you can update the system metadata using the updateSystemMetadata function, which takes the mn instance, your PID of interest, and the edited sysmeta object as arguments. updateSystemMetadata(mn, pid, sysmeta) 4.2.1 Exercise Read the system metadata in from the data file you uploaded in the previous chapter Check to make sure the fileName and formatId are set correctly Update the system metadata if necessary. 4.3 Editing sysmeta on lots of files If you have a lot of files that all need a similar change, you can use a for loop or the apply functions to efficiently make all of your changes. Say you need to change all of the format IDs of data objects in a package so that they are text/csv. You could do this: PID &lt;- &#39;some_pid&#39; ids &lt;- get_package(mn, PID) #get all pids in a package data_pids &lt;- unlist(ids$data) for (i in 1:length(data_pids)){ #run for loop over all of the data ids sysmeta &lt;- getSystemMetadata(mn, data_pids[i]) sysmeta@formatId &lt;- &quot;text/csv&quot; updateSystemMetadata(mn, data_pids[i], sysmeta) } 4.4 Identifiers and sysmeta Importantly, changing the system metadata does NOT necessitate a change in the PID of an object. This is because changes to the system metadata do not change the object itself, they are only changing the description of the object (although ideally the system metadata is accurate when an object is first published). "],
["editing-eml-in-r.html", "Chapter 5 Editing EML in R 5.1 Introduction 5.2 Making edits 5.3 Validating and writing the EML 5.4 Dealing with unusual cases 5.5 Exercise", " Chapter 5 Editing EML in R 5.1 Introduction This chapter is a practical tutorial for using the EML package to read, edit, write, and validate EML documents. Much of this information can be also be found in the vignettes for the EML package. First, load in some packages. library(EML) library(XML) library(digest) library(arcticdatautils) Set a path to a local copy of the EML that you would like to edit, and read the EML into R. path1 &lt;- &#39;glacier_metadata.xml&#39; eml &lt;- read_eml(path1) When using the EML R package, you will usually be working with objects of class eml. For example, we can find out what type of object we just created when we ran the read_eml function: class(eml) ## [1] &quot;eml&quot; ## attr(,&quot;package&quot;) ## [1] &quot;EML&quot; This eml object represents a complete EML document (as in: the .xml file) and all of the information inside the .xml file can be viewed and edited with the EML package. The eml object has what are called “slots,” each slot representing an element of the EML document such as “Title” or “Creator” and you will use these “slots” when working with your eml object. You can find out what slots you have available with the slotNames function: sort(slotNames(eml)) ## [1] &quot;.Data&quot; &quot;access&quot; &quot;additionalMetadata&quot; ## [4] &quot;citation&quot; &quot;dataset&quot; &quot;lang&quot; ## [7] &quot;packageId&quot; &quot;protocol&quot; &quot;schemaLocation&quot; ## [10] &quot;scope&quot; &quot;slot_order&quot; &quot;software&quot; ## [13] &quot;system&quot; You can access information in one of these slots by adding an @ symbol at the end of the variable you want to view the slots of and hitting TAB (e.g., eml@&lt;TAB&gt;): RStudio Autocompletion You will notice that the list of slots on the eml object doesn’t include common EML things such as the dataset’s title or the creators. This is because EML (the XML) and the EML R package are both hierarchical. The (incomplete) EML XML for dataset with a title would look something like this: &lt;eml&gt; &lt;dataset&gt; &lt;title&gt;My title goes here...&lt;/title&gt; &lt;/dataset&gt; &lt;/eml You can see that the title is nested inside the dataset and the dataset is nested inside the root eml element of the document. Because the title is nested inside the dataset element, it will be a slot on eml@dataset instead of an slot of the main eml object: sort(slotNames(eml@dataset)) ## [1] &quot;.Data&quot; &quot;abstract&quot; &quot;additionalInfo&quot; ## [4] &quot;alternateIdentifier&quot; &quot;associatedParty&quot; &quot;contact&quot; ## [7] &quot;coverage&quot; &quot;creator&quot; &quot;dataTable&quot; ## [10] &quot;distribution&quot; &quot;id&quot; &quot;intellectualRights&quot; ## [13] &quot;keywordSet&quot; &quot;lang&quot; &quot;language&quot; ## [16] &quot;maintenance&quot; &quot;metadataProvider&quot; &quot;methods&quot; ## [19] &quot;otherEntity&quot; &quot;project&quot; &quot;pubDate&quot; ## [22] &quot;publisher&quot; &quot;pubPlace&quot; &quot;purpose&quot; ## [25] &quot;references&quot; &quot;schemaLocation&quot; &quot;scope&quot; ## [28] &quot;series&quot; &quot;shortName&quot; &quot;slot_order&quot; ## [31] &quot;spatialRaster&quot; &quot;spatialVector&quot; &quot;storedProcedure&quot; ## [34] &quot;system&quot; &quot;title&quot; &quot;view&quot; Remember, you can just type eml@dataset into your console and hit to see this list of slot names. The slotNames function is used for demonstration purposes here. Slots can be nested in each other and are all based on the EML schema. Typing in the name of your eml object (in this case, eml, the name of the result of read_eml) and hitting &lt;RETURN&gt; in the console will print the entire EML onto the screen: eml Similarly, you can also view different elements of the EML by drilling down into the structure using the @ functionality. This would just print the dataset portion of the EML: eml@dataset Going even deeper, this command will print the title element, which is within the dataset element, of the EML document: eml@dataset@title ## An object of class &quot;ListOftitle&quot; ## [[1]] ## &lt;title&gt;Subglacial conduit fluid dynamics simulation, Svalbard, Norway&lt;/title&gt; Notice than when the last line prints, it doesn’t just print a string - it returns “An object of class ListOftitle.” The EML is composed of different classes of objects that make up these slots. If a slot can have multiple items, it may require an input of a list of a class. You can continue digging down into the EML using subsetting techniques for the S4 object class, with syntax that looks like this: eml@dataset@title@.Data[[1]]@.Data ## [1] &quot;Subglacial conduit fluid dynamics simulation, Svalbard, Norway&quot; Notice that this actually prints a character string. If you want to change the title, you could just assign this a new character string value, like this: eml@dataset@title@.Data[[1]]@.Data &lt;- &#39;This is my new title&#39; However, this isn’t best method to edit the EML unless you are an expert both in S4 objects and in the EML schema, since the nesting and lists of elements can get very complex. Instead, the EML package is used to create new objects of particular classes to put into the document, using the function new. The new function has a format that looks like newobject &lt;- new('class', arguments...). It can be a hard function to use because “class” has to be set to a specific name and the argument structure will vary depending on what the slots are in that class. Because the function is so general, the ?new help is not very helpful. In the EML package, a good guess to what the name of the class will be is a slot name (such as “title”). You can explore what slots are available within an object class by creating a new, empty object like this: test_title &lt;- new('title') and using the R autocomplete functionality on test_title@. Not that the slots lang, slot_order, schemaLocation, and .Data will always be present, and are set by the EML package automatically according to the required EML and XML schema. The other options will tell you what the arguments following the class name should be. In the case of the title class, the only option is value. So, to change the title, you might at first try do something like this: eml@dataset@title&lt;- new('title', .Data = 'This is my new title') Unfortunately it returns an error. The error reads: Error in (function (cl, name, valueClass) :assignment of an object of class “title” is not valid for @‘title’ in an object of class “dataset” ; is(value, &quot;ListOftitle&quot;) is not TRUE The error says it cannot assign an object of class “title” to eml@dataset@title, and that there is not a value for ListOftitle. So this means that you have to assign to @title an object of class ListOftitle, which is composed of objects of class title. Even though ListOftitle is it’s own kind of class, we can avoid using the new function again here by simply putting the title object in a vector using the c() function. title &lt;- new(&#39;title&#39;, .Data = &#39;This is my new title&#39;) eml@dataset@title &lt;- c(title) This seems kind of cumbersome, creating first a new title object, and then a list of title, especially since in most cases an EML will only have one title. However, say for some reason you need two titles - you can then do this: title_second &lt;- new(&#39;title&#39;, .Data = &#39;This dataset has two titles&#39;) eml@dataset@title &lt;- c(title, title_second) eml@dataset@title ## An object of class &quot;ListOftitle&quot; ## [[1]] ## &lt;title&gt;This is my new title&lt;/title&gt; ## ## [[2]] ## &lt;title&gt;This dataset has two titles&lt;/title&gt; This functionality will prove very useful with other elements (such as dataTable), where there is usually more than one of the same element. 5.2 Making edits 5.2.1 Setting attributes Since attribute information has to be added to the metadata, we’ll cover attributes first. 5.2.1.1 Building the attribute table First you need to generate a dataframe with attribute information. This dataframe has rows that are attributes, and the following columns: attributeName: The name of the attribute as listed in the csv. Required. attributeDefinition: Longer description of the attribute. Required. measurementScale: One of: nominal, ordinal, dateTime, ratio, interval. Required. nominal: unordered categories or text. eg: (Male, Female) or (Yukon River, Kuskokwim River) ordinal: ordered categories. eg: Low, Medium, High dateTime: date or time values from the Gregorian calendar. eg: 01-01-2001 ratio: measurement scale with a meaningful zero point. eg: 200 Kelvin is half as hot as 400 Kelvin, 1.2 metersPerSecond is twice as fast as 0.6 metersPerSecond. interval: values from a scale with equidistant points, where the zero point is arbitrary. eg: 12.2 degrees Celsius, 21 degrees Latitude domain: One of: textDomain, enumeratedDomain, numericDomain, dateTimeDomain. Required. textDomain: text that is free-form, or matches a pattern enumeratedDomain: text that belongs to a defined list of codes and definitions. eg: CASC = Cascade Lake, HEAR = Heart Lake dateTimeDomain: dateTime attributes numericDomain: attributes that are numbers (either ratio or interval) formatString: Required for dateTimeDomain, NA otherwise. Format string for dates, eg “MM/DD/YYYY”. definition: Required for textDomain, NA otherwise. Definition for attributes that are a character string, matches attribute definition in most cases. unit: Required for numericDomain, NA otherwise. Unit string. If the unit is not a standard unit, a warning will appear when you create the attribute list, saying that it has been forced into a custom unit. Use caution here to make sure the unit really needs to be a custom unit. A list of standard units can be found here: https://knb.ecoinformatics.org/#external//emlparser/docs/eml-2.1.1/./eml-unitTypeDefinitions.html#StandardUnitDictionary numberType: Required for numericDomain, NA otherwise. Options are “real”, “natural”, “whole”, “integer”. real: positive and negative fractions and non fractions (…-1,-0.25,0,0.25,1…) natural: non-zero positive counting numbers (1,2,3…) whole: positive counting numbers and zero (0,1,2,3…) integer: positive and negative counting numbers and zero (…-2,-1,0,1,2…) missingValueCode: Code for missing values (eg: ‘-999’, ‘NA’, ‘NaN’). NA otherwise. Note that an NA missing value code should be a string, ‘NA’, and numbers should also be strings, ‘-999.’ missingValueCodeExplanation: Explanation for missing values, NA if no missing value code exists. attributes1 &lt;- data.frame( attributeName = c(&#39;Date&#39;, &#39;Location&#39;, &#39;Region&#39;,&#39;Sample_No&#39;, &#39;Sample_vol&#39;, &#39;Salinity&#39;, &#39;Temperature&#39;, &#39;sampling_comments&#39;), attributeDefinition = c(&#39;Date sample was taken on&#39;, &#39;Location code representing location where sample was taken&#39;,&#39;Region where sample was taken&#39;, &#39;Sample number&#39;, &#39;Sample volume&#39;, &#39;Salinity of sample in PSU&#39;, &#39;Temperature of sample&#39;, &#39;comments about sampling process&#39;), measurementScale = c(&#39;dateTime&#39;, &#39;nominal&#39;,&#39;nominal&#39;, &#39;nominal&#39;, &#39;ratio&#39;, &#39;ratio&#39;, &#39;interval&#39;, &#39;nominal&#39;), domain = c(&#39;dateTimeDomain&#39;, &#39;enumeratedDomain&#39;,&#39;enumeratedDomain&#39;, &#39;textDomain&#39;, &#39;numericDomain&#39;, &#39;numericDomain&#39;, &#39;numericDomain&#39;, &#39;textDomain&#39;), formatString = c(&#39;MM-DD-YYYY&#39;, NA,NA,NA,NA,NA,NA,NA), definition = c(NA,NA,NA,&#39;Sample number&#39;, NA, NA, NA, &#39;comments about sampling process&#39;), unit = c(NA, NA, NA, NA,&#39;milliliter&#39;, &#39;dimensionless&#39;, &#39;celsius&#39;, NA), numberType = c(NA, NA, NA,NA, &#39;real&#39;, &#39;real&#39;, &#39;real&#39;, NA), missingValueCode = c(NA, NA, NA,NA, NA, NA, NA, &#39;NA&#39;), missingValueCodeExplanation = c(NA, NA, NA,NA, NA, NA, NA, &#39;no sampling comments&#39;), stringsAsFactors = FALSE) Typing this out in R can be a bit of a pain, so you can import a table made in another program (such as Excel) as your attribute table - just make sure that rows are attributes and column names match the column names as listed above exactly (case is important). attributes1 &lt;- read.csv('~/arctic-data/docs/training/EML/LakeSampleData.csv', stringsAsFactors = F) 5.2.1.2 Defining enumerated domains For attributes that are enumerated domains, a second table is needed with three columns: attributeName, code, and definition. attributeName is repeated for all codes belonging to a common attribute. To make things a little easier and less repetitve, coding wise, codes can be defined using named character vectors and then converted to a data frame. In this example, there are two enumerated domains in the attribute list - “Location” and “Region” Location &lt;- c(CASC = &#39;Cascade Lake&#39;, CHIK = &#39;Chikumunik Lake&#39;, HEAR = &#39;Heart Lake&#39;, NISH = &#39;Nishlik Lake&#39; ) Region &lt;- c(W_MTN = &#39;West region, locations West of Eagle Mountain&#39;, E_MTN = &#39;East region, locations East of Eagle Mountain&#39;) The definitions are then written into a dataframe using the names of the named character vectors, and their definitions. factors1 &lt;- rbind(data.frame(attributeName = &#39;Location&#39;, code = names(Location), definition = unname(Location)), data.frame(attributeName = &#39;Region&#39;, code = names(Region), definition = unname(Region))) factors1 ## attributeName code definition ## 1 Location CASC Cascade Lake ## 2 Location CHIK Chikumunik Lake ## 3 Location HEAR Heart Lake ## 4 Location NISH Nishlik Lake ## 5 Region W_MTN West region, locations West of Eagle Mountain ## 6 Region E_MTN East region, locations East of Eagle Mountain This table can also be generated using a different program, such as Excel, and imported to R as a .csv, similar to what can be done with the attribute table. 5.2.1.3 Generating the attribute list and data table Next the attributeList is generated from the attributes and the factors using the function set_attributes. This puts all of the information from the attribute data.frame and the factor data.frame defining the enumerated domains into the slotted EML schema. attributeList1 &lt;- set_attributes(attributes1, factors = factors1) Now the physical aspects of the data table, like its name, identifier (PID), header lines, and delimiter, need to be described. The function set_physical does this. See ?set_physical for more options on what can be set in the physical element. One of the more important items to set here is the URL, which points to the newest version of the data object using the object’s PID. id1 &lt;- &#39;PID1&#39; #this should be an actual PID path &lt;- &#39;~/arctic-data/docs/training/EML/LakeSampleData.csv&#39; #path to data table physical1 &lt;- set_physical(&#39;LakeSampleData.csv&#39;, id = id1, size = as.character(file.size(path)), sizeUnit = &#39;bytes&#39;, authentication = digest(path, algo=&quot;sha1&quot;, serialize=FALSE, file=TRUE), authMethod = &#39;SHA-1&#39;, numHeaderLines = &#39;1&#39;, fieldDelimiter = &#39;,&#39;, url = paste0(&#39;https://cn.dataone.org/cn/v2/resolve/&#39;, id1)) If the object is already on the Arctic Data Center, the physical section is very easy to write using its PID and the pid_to_eml_physical function: id1 &lt;- &#39;PID1&#39; #this should be an actual PID cn &lt;- CNode(&#39;PROD&#39;) mn &lt;- getMNode(cn,&#39;urn:node:ARCTIC&#39;) physical1 &lt;- pid_to_eml_physical(mn, id1) REMEMBER, if we are working in the test environment, we set it this way: cn &lt;- CNode(&#39;STAGING&#39;) mn &lt;- getMNode(cn,&#39;urn:node:mnTestARCTIC&#39;) Reminder: Be very careful about what you publish on the production node (PROD, or arcticdata.io). This node should NEVER be used to publish test or training datasets. While going through training for the first time you should be using the test environment (STAGING, or test.articdata.io). The physical1 and attributeList1 elements are then used to create the dataTable, along with the name of the dataTable and its description. dataTable1 &lt;- new(&#39;dataTable&#39;, entityName = &#39;LakeSampleData.csv&#39;, entityDescription = &#39;Water sample temperature and salinity from the Eagle Mountain region&#39;, physical = physical1, attributeList = attributeList1) 5.2.1.4 Adding a second dataTable If the metadata document describes multiple Data Objects, a new set of attributes, attribute list, physical description, and dataTable can be created just as in the example above. attributes2 &lt;- data.frame(attributeName = c(&#39;Time&#39;, &#39;Wind_Speed&#39;), attributeDefinition = c(&#39;Date and time of wind speed reading&#39;, &#39;Measured wind speed&#39;), measurementScale = c(&#39;dateTime&#39;, &#39;ratio&#39;), domain = c(&#39;dateTimeDomain&#39;, &#39;numericDomain&#39;), formatString = c(&#39;YYYY-MM-DD hh:mm:ss&#39;, NA), definition = c(NA, NA), unit = c(NA, &#39;metersPerSecond&#39;), numberType = c(NA, &#39;real&#39;), missingValueCode = c(NA, NA), codeExplanation = c(NA, NA), stringsAsFactors = FALSE) attributeList2 &lt;- set_attributes(attributes2) id2 &lt;- &#39;PID2&#39; physical2 &lt;- pid_to_eml_physical(mn, id2) dataTable2 &lt;- new(&#39;dataTable&#39;, entityName = &#39;EagleMtnWindData.csv&#39;, entityDescription = &#39;Wind data from Eagle Mountain&#39;, physical = physical2, attributeList = attributeList2) Now both dataTable1 and dataTable2 are added to the original EML using the c() function. eml@dataset@dataTable &lt;- c(dataTable1, dataTable2) 5.2.1.5 Defining custom units If you get a warning message about your units not being in the standard unit list, a custom unit list needs to be created and added to the additionalMetadata slot of the EML. First we create a custom unit data frame with columns id (the name of the unit, as it appears in the attribute table), unitType, and parentSI. Each custom unit has a row in the data frame. To determine what the unitType and parentSI are for each unit, it may be helpful to reference the list of custom units. This list can easily be viewed in your R console using these commands: standardUnits &lt;- get_unitList() View(standardUnits$units) The following lines create the standard unit data table, use the function set_unitList to slot it into the EML, and then add it as addionalMetadata to the EML. custom_units &lt;- data.frame(id = c(&#39;horsepower&#39;, &#39;gallonPerMinute&#39;), unitType = c(&#39;power&#39;, &#39;volumetricRate&#39;), parentSI = c(&#39;watt&#39;, &#39;litersPerSecond&#39;)) unitlist &lt;- set_unitList(custom_units) eml@additionalMetadata &lt;- c(as(unitlist, &quot;additionalMetadata&quot;)) 5.2.2 Setting other entities 5.2.2.1 Removing other entities In cases where the data package was submitted originally via the registry, the original EML usually has the data tables described as “other entity” elements in the EML. This information is now redundant since we created data table elements describing these objects. Remove the other entities by replacing the other entity element in the EML with a ListOfotherEntity object that consists of an empty list. eml@dataset@otherEntity &lt;- new(&#39;ListOfotherEntity&#39;, list()) 5.2.2.2 Adding other entities for files that aren’t uploaded yet There are times, however, when it may be necessary to create other entity tables for objects that are not described using a data table. Examples of these can include: R scripts, large NetCDF file directories, or audio/image files. Adding other entities is easy as long as you have paths to the files on the server and PIDs. This workflow is best if you are uploading files to the ADC yourself. First get the list of pathnames for the files you are going to upload. paths &lt;- list.files(&quot;/home/sjclark/EML_learning/&quot;, full.names = TRUE) Then generate PIDs for those files. These will be the data PIDs you use in publish_update, but note that you may have other data PIDs for data objects that are not other entities. pids &lt;- vapply(paths, function(x) { paste0(&quot;urn:uuid:&quot;, uuid::UUIDgenerate()) }, &quot;&quot;) This will guess the format ID from the file extension. These should be checked afterwards and potentially changed. format_ids &lt;- guess_format_id(paths) Finally, a data frame is created with all of that information, and that information is added into the EML using eml_add_entities. entity_df &lt;- data.frame(type = &#39;otherEntity&#39;, path = paths, pid = pids, format_id = format_ids, stringsAsFactors = FALSE) eml &lt;- eml_add_entities(eml, entity_df) 5.2.2.3 Adding other entities for files that are already uploaded If you are trying to describe data objects that are already on the ADC, you can utilize lapply along with get_package to easily write the EML otherEntity elements. In this case, there are data tables and other entity type elements mixed in, so the otherEntity elements (non-csvs) are picked out by hand using indexes. The numbers in the call otherEnts &lt;- pkg$data[c(5,6,9,10)] will change depending on your dataset. pid = &#39;doi:10.18739/A2408F&#39; pkg &lt;- get_package(mn, pid, file_names = T) otherEnts &lt;- pkg$data[c(5,6,9,10)] #select only `otherEntity` PIDs after viewing `pkg$data` contents eml@dataset@otherEntity &lt;- new(&quot;ListOfotherEntity&quot;, pid_to_eml_other_entity(mn, otherEnts)) 5.2.3 Setting coverages Sometimes EML documents may lack coverage information describing the temporal, geographic, or taxonomic coverage of a dataset. This example shows how to create coverage information from scratch, or replace an existing coverage element with an updated one. You can view the current coverage (if it exists) by entering eml@dataset@coverage into the console. Here the coverage, including temporal, taxonomic, and geographic coverages, is defined using set_coverage. coverage &lt;- set_coverage(beginDate = &#39;2012-01-01&#39;, endDate = &#39;2012-01-10&#39;, sci_names = c(&#39;exampleGenus exampleSpecies1&#39;, &#39;exampleGenus ExampleSpecies2&#39;), geographicDescription = &quot;The geographic region covers the lake region near Eagle Mountain.&quot;, west = -154.6192, east = -154.5753, north = 68.3831, south = 68.3619) eml@dataset@coverage &lt;- coverage You can also set multiple geographic (or temporal) coverages. Here is an example of how you might set two geographic coverages. geocov1 &lt;- new(&quot;geographicCoverage&quot;, geographicDescription = &quot;The geographich region covers area 1&quot;, boundingCoordinates = new(&quot;boundingCoordinates&quot;, northBoundingCoordinate = new(&quot;northBoundingCoordinate&quot;, 68), eastBoundingCoordinate = new(&quot;eastBoundingCoordinate&quot;, -154), southBoundingCoordinate = new(&quot;southBoundingCoordinate&quot;, 67), westBoundingCoordinate = new(&quot;westBoundingCoordinate&quot;, -155))) geocov2 &lt;- new(&quot;geographicCoverage&quot;, geographicDescription = &quot;The geographich region covers area 2&quot;, boundingCoordinates = new(&quot;boundingCoordinates&quot;, northBoundingCoordinate = new(&quot;northBoundingCoordinate&quot;, 65), eastBoundingCoordinate = new(&quot;eastBoundingCoordinate&quot;, -155), southBoundingCoordinate = new(&quot;southBoundingCoordinate&quot;, 64), westBoundingCoordinate = new(&quot;westBoundingCoordinate&quot;, -156))) coverage &lt;- set_coverage(beginDate = &#39;2012-01-01&#39;, endDate = &#39;2012-01-10&#39;, sci_names = c(&#39;exampleGenus exampleSpecies1&#39;, &#39;exampleGenus ExampleSpecies2&#39;)) eml@dataset@coverage@geographicCoverage &lt;- c(geocov1, geocov2) 5.2.4 Setting methods The methods tree in the EML section has many different options, visible in the schema. You can create new elements in the methods tree by following the schema and using the “new” command. Remember you can explore possible slots within an element by creating an empty object of the class you are trying to create. For example, method_step &lt;- new('methodStep'), and using autocomplete on method_step@. One very simple, and potentially useful way to add methods to an EML that have no methods at all is adding them via a word document. An example is shown below: methods1 &lt;- set_methods(&#39;methods_doc.docx&#39;) eml@dataset@methods &lt;- methods1 If you want to make minor changes to existing method information that has a lot of nested elements, your best bet may be to edit the EML in a text editor, otherwise there is a risk of accidentally overwriting nested elements with blank object classes, therefore losing method information. 5.2.5 Adding people To add people, with their addresses, you need to add addresses as their own object class, which you then add to the contact, creator, or associated party classes. NCEASadd &lt;- new(&#39;address&#39;, deliveryPoint = &#39;735 State St #300&#39;, city = &#39;Santa Barbara&#39;, administrativeArea = &#39;CA&#39;, postalCode = &#39;93101&#39;) The creator, contact, and associated party classes can easily be created using functions from the arcticdatautils package. Here, we use eml_creator to set our dataset creator. JC_creator &lt;- eml_creator(&quot;Jeanette&quot;, &quot;Clark&quot;, &quot;NCEAS&quot;, &quot;jclark@nceas.ucsb.edu&quot;, phone = &#39;123-456-7890&#39;, userId = &#39;https://orcid.org/WWWW-XXXX-YYYY-ZZZZ&#39;, address = NCEASadd) eml@dataset@creator &lt;- c(JC_creator) Similarly, we can set the contacts. In this case, there are two, so we set eml@dataset@contact as a ListOfcontact, which contains both of them. JC_contact &lt;- eml_contact(&quot;Jeanette&quot;, &quot;Clark&quot;, &quot;NCEAS&quot;, &quot;jclark@nceas.ucsb.edu&quot;, phone = &#39;123-456-7890&#39;, userId = &#39;https://orcid.org/WWWW-XXXX-YYYY-ZZZZ&#39;, address = NCEASadd) JG_contact &lt;- eml_contact(&quot;Jesse&quot;, &quot;Goldstein&quot;, &quot;NCEAS&quot;, &quot;jclark@nceas.ucsb.edu&quot;, phone = &#39;123-456-7890&#39;, userId = &#39;https://orcid.org/WWWW-XXXX-YYYY-ZZZZ&#39;, address = NCEASadd) eml@dataset@contact &lt;- c(JC_contact, JG_contact) Finally, the associated parties are set. Note that associated parties MUST have a role defined, unlike creator or contact. JG_ap &lt;- eml_associated_party(&quot;Jesse&quot;, &quot;Goldstein&quot;, &quot;NCEAS&quot;, &quot;jclark@nceas.ucsb.edu&quot;, phone = &#39;123-456-7890&#39;, address = NCEASadd, userId = &#39;https://orcid.org/WWWW-XXXX-YYYY-ZZZZ&#39;, role = &quot;metaataProvider&quot;) eml@dataset@associatedParty &lt;- c(JG_ap) 5.3 Validating and writing the EML The last step is to write and validate the EML. For time-saving purposes, first you write the EML: path2 &lt;- '/path/to/new/eml/Lake Physical Properties Data.xml' write_eml(eml, path2) Now, you validate the eml. The EML validation step indicates whether the EML that you’ve created is valid both with regard to EML and XML schema. Hopefully it returns TRUE. eml_validate(path2) If the EML validate returns FALSE, it is accompanied by an error that will be in this format: 69.0: Element 'boundingCoordinates': This element is not expected. Expected is one of ( geographicDescription, references ). This error essentially says, the EML validator reached the slot boundingCoordinates but did not expect this element. Instead it expected either geographicDescription or references. Referring to the schema maps (eg: https://knb.ecoinformatics.org/emlparser/docs/eml-2.1.1/eml-coverage.png) you can see that before bounding coordinates, there must be a geographic description. The fix would be to return to your definiton of the geographicCoverage, and insert a geographicDescription into the geographicCoverage object (ie:geocov1 &lt;- new('geographicCoverage', geographicDescription = 'Description here',...)). 5.4 Dealing with unusual cases Typically, the biggest issue most datateam members have using the EML package is trying to add multiple values within a slot. In some cases, like adding multiple dataTable instances or multiple creator instances, this is easy, as is shown above. Other times, particularly if the slot is nested within a slot that we use a helper function for (like set_attributes), it can get a little more challenging. By delving a little deeper into the EML S4 class of objects though, you can resolve most problems relatively easily. Any slot that allows for a list of objects, as listed in the EML schema (https://knb.ecoinformatics.org/#external//emlparser/docs/eml-2.1.1/index.html), will have a class called the slot name, in addition to a class called ListOfslotName. You can use the new function to create an object of the slot itself and a list of objects within that slot. For example: m1 &lt;- new(&#39;missingValueCode&#39;, code = &#39;NA&#39;, codeExplanation = &#39;No data taken&#39;) m2 &lt;- new(&#39;missingValueCode&#39;, code = &#39;-999&#39;, codeExplanation = &#39;Sensor malfunctioned&#39;) codes &lt;- new(&#39;ListOfmissingValueCode&#39;, list(m1, m2)) In this case, this slot is nested within a part of the EML that we typically construct with a helper function - so how do we actually get this list of missing value codes into the attribute table? One strategy would be to construct the attribute list without the missing value code information for the attribute with mutliple attributes, and then insert the ListOfmissingValueCode into it. So, building our attribute list as normal, attributes1 &lt;- read.csv(&#39;~/arctic-data/docs/training/EML/LakeSampleData.csv&#39;, stringsAsFactors = F) attlist1 &lt;- set_attributes(attrbutes1) You can then dig down into the individual attributes using the @ functionality. Note that all of the attributes are stored as a ListOfattributes object. You can view a single value within this list by calling attlist1@attributes@.Data[[n]] where n represents the index of the list you want to view. attlist1@attribute@.Data[[1]] &lt;attribute system=&quot;uuid&quot;&gt; &lt;attributeName&gt;Date&lt;/attributeName&gt; &lt;attributeDefinition&gt;Date sample was taken on&lt;/attributeDefinition&gt; &lt;measurementScale&gt; &lt;dateTime&gt; &lt;formatString&gt;MM-DD-YYYY&lt;/formatString&gt; &lt;/dateTime&gt; &lt;/measurementScale&gt; &lt;/attribute&gt; You can also run commands on attlist1@attribute@.Data just like you would any other list. One useful one is length(attlist1@attribute@.Data) which will return the number of items in the list. So to insert the list of missing value codes, you’ll need to navigate to the index in the list of the attribute that you need to modify, and then dig even deeper into the slots. Don’t forget that the tab key is your friend and can help you find slot names! So if you know that you need to change the 6th attribute in the list, you can type the following to see what is in there now: attlist1@attribute@.Data[[6]]@missingValueCode An object of class &quot;ListOfmissingValueCode&quot; [[1]] &lt;missingValueCode/&gt; Now to add the list, you’ll have to execute the following (repeating lines for clarity): m1 &lt;- new(&#39;missingValueCode&#39;, code = &#39;NA&#39;, codeExplanation = &#39;No data taken&#39;) m2 &lt;- new(&#39;missingValueCode&#39;, code = &#39;-999&#39;, codeExplanation = &#39;Sensor malfunctioned&#39;) codes &lt;- new(&#39;ListOfmissingValueCode&#39;, list(m1, m2)) attlist1@attribute@.Data[[6]]@missingValueCode &lt;- codes Now, if you view this attribute again you should see the mutliple missing value codes: &gt; attlist1@attribute@.Data[[6]]@missingValueCode An object of class &quot;ListOfmissingValueCode&quot; [[1]] &lt;missingValueCode&gt; &lt;code&gt;NA&lt;/code&gt; &lt;codeExplanation&gt;No data taken&lt;/codeExplanation&gt; &lt;/missingValueCode&gt; [[2]] &lt;missingValueCode&gt; &lt;code&gt;-999&lt;/code&gt; &lt;codeExplanation&gt;Sensor malfunctioned&lt;/codeExplanation&gt; &lt;/missingValueCode&gt; What I showed above is a pretty specific example (which conveniently is also one of our more common “rare cases”), but there are some general strategies you can employ to figure out how to insert other objects that need to be deeply nested into an EML. Create an empty test object: example: test &lt;- new('abstract') Use the @ and tab functionality to explore slots in that object: example: test@para What are all these slots??: Remember that the schemaLocation, lang, slot_order, and section slots are related to the schema and are not actual slots which contain values you can edit. .Data: if you run into a .Data using the tab key, that means that the slot can be part of a list. Use indexing to access individual items in a list: example: test@para@.Data[[2]] Index out of bounds: If you receive an “index out of bounds” error that means you are trying to access an index beyond what exists in the list. For example, if there are 4 paragraphs in the abstract, and you write test@para@.Data[[5]] you will recieve this error. Use the schema!: https://knb.ecoinformatics.org/#external//emlparser/docs/eml-2.1.1/index.html 5.5 Exercise Read in the EML that you updated in the previous exercise into R Use the eml package to replace the exsiting dataTable slot with a new dataTable object with an attribute list and physical section you wrote in R Write and validate your EML Update your package with the new EML using publish_update Use the checklist Make edits where necessary, and publish an update if needed "]
]
